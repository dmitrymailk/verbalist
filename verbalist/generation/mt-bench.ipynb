{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.29s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from verbalist.generation.generation_utils import (\n",
    "    VerbalistConversation,\n",
    "    generate,\n",
    "    VerbalistOpenchatConversation,\n",
    ")\n",
    "\n",
    "\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v5/checkpoint-850/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v6/checkpoint-900/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_30b_v2/checkpoint-5650/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_13b_v2/checkpoint-6900/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v6/checkpoint-2700/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_30b_v3/checkpoint-400/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v7/checkpoint-24800/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_1.1b_v1/checkpoint-13850/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_13b_v5/checkpoint-12500/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_30b_v3/checkpoint-6700/adapter_model/\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v10/checkpoint-1650/adapter_model\"\n",
    "# weights_path = (\n",
    "#     \"verbalist/model/models/mistral_7b_oasst1_dolly_v1/checkpoint-350/adapter_model/\"\n",
    "# )\n",
    "weights_path = \"verbalist/model/models/verbalist_7b_v9/checkpoint-800/adapter_model/\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_13b_v5/checkpoint-5200/adapter_model\"\n",
    "# tokenizer_path = \"verbalist/model/models/verbalist_7b_v6/\"\n",
    "# tokenizer_path = \"verbalist/model/models/mistral_7b_oasst1_dolly_v1/\"\n",
    "tokenizer_path = \"verbalist/model/models/verbalist_7b_v9/\"\n",
    "\n",
    "\n",
    "load_in_8bit = \"13b\" in weights_path or \"7b\" in weights_path\n",
    "load_in_4bit = \"30b\" in weights_path\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=load_in_8bit,\n",
    "    # load_in_4bit=load_in_4bit,\n",
    "    torch_dtype=torch.float16,\n",
    "    # load_in_8bit_fp32_cpu_offload=True,\n",
    "    device_map={\"\": 0},\n",
    "    # device_map='cpu',\n",
    "    # device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    "    # load_in_8bit_fp32_cpu_offload=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    # torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT <s> system\n",
      "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
      "<s> user\n",
      "Сочинение на тему проблемы кризиса русской философии </s> \n",
      "<s> bot\n",
      "В последние годы в России возникли серьезные проблемы в области философии. Это связано с тем, что многие российские философы не могут найти ответы на вопросы о будущем страны и ее развитии. В результате это приводит к тому, что философия становится все более и более изолированной от реальности и постепенно теряет свою значимость для общества.\n",
      "\n",
      "Одна из основных причин этого явления заключается в том, что российская философия не может предоставить достаточно глубоких и обоснованных ответов на вопросы о будущем страны. Философы часто выступают как критики, но не предлагают альтернативных подходов к решению проблем. Вместо этого они продолжают использовать старые идеи и концепции, которые уже не соответствуют современным реалиям.\n",
      "\n",
      "Кроме того, российская философия не может предоставить достаточно эффективных методов решения проблем, которые возникают в обществе. Она не может предложить новые подходы к решению социальных, политических и экономических проблем, которые нуждаются в решении. Вместо этого она продолжает использовать старые методы, которые уже не работают.\n",
      "\n",
      "Наконец, российская философия не может предоставить достаточно эффективных методов обучения и воспитания молодого поколения. Она не может обеспечить достаточно высокий уровень образования и культуры, который необходим для успешного развития страны. Вместо этого она продолжает использовать старые методы обучения, которые уже не работают.\n",
      "\n",
      "В целом, российская философия находится в кризисе, так как она не может предоставить достаточно эффективных методов решения проблем, которые возникают в обществе. Она не может предложить новые подходы к решению социальных, политических и экономических проблем, которые нуждаются в решении. Вместо этого она продолжает использовать старые методы, которые уже не работают. Это приводит к тому, что философия становится все более и более изолированной от реальности и постепенно теряет свою значимость для общества.\n"
     ]
    }
   ],
   "source": [
    "conversation = VerbalistConversation(bot_token_id=12435)\n",
    "# conversation = VerbalistOpenchatConversation()\n",
    "\n",
    "conversation.add_user_message(\"Сочинение на тему проблемы кризиса русской философии\")\n",
    "\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "print(\"PROMPT\", prompt)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    max_new_tokens=4000,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "\n",
    "# print(inp)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom mistral open orca ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "# model_name = \"dim/mistral-open-orca-ru-4600-step\"\n",
    "# model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "model_name = \"dim/tiny-llama-2T-open-orca-ru-10000-step\"\n",
    "tokenizer_path = model_name\n",
    "# model_name = \"/home/kosenko/open_orca_ru/axolotl/open_orca_ru/checkpoint-10000\"\n",
    "# tokenizer_path = \"/home/kosenko/open_orca_ru/axolotl/open_orca_ru/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "# from transformers import AutoModelForCausalLM;AutoModelForCausalLM.from_pretrained(\"dim/mistral-open-orca-ru-3000-step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Трава зеленая, потому что она поглощает солнечный свет и преобразует его в химическую энергию посредством процесса, называемого фотосинтезом. Во время фотосинтеза растения используют углекислый газ и воду для производства глюкозы (разновидность сахара) и кислорода. Глюкоза используется растением для получения энергии, а кислород выделяется в атмосферу.\n",
      "\n",
      "Трава содержит большое количество хлорофилла, пигмента, который поглощает солнечный свет и преобразует его в химическую энергию. Хлорофилл отвечает за зеленый цвет травы и других растений.\n",
      "\n",
      "Таким образом, трава зеленая, потому что она поглощает солнечный свет и преобразует его в химическую энергию посредством фотосинтеза, что приводит к производству глюкозы и кислорода.\n",
      "Небо кажется голубым из-за рассеяния солнечного света атмосферой Земли. Солнечный свет состоит из широкого спектра цветов, и когда он проходит через атмосферу Земли, он рассеивается на части, которые мы можем видеть. Это рассеяние заставляет свет с разной длиной волны рассеиваться и создавать голубоватый оттенок.\n",
      "\n",
      "Когда солнечный свет проходит через атмосферу Земли, он взаимодействует с молекулами воздуха. Эти молекулы воздуха поглощают и отражают часть солнечного света, в результате чего свет рассеивается на части, которые мы можем видеть. Часть света, которая проходит через атмосферу и достигает наших глаз, имеет более короткие длины волн, такие как красный и оранжевый, и отражается обратно к нашим глазам. Часть света с большей длиной волны, например, синий и зеленый, поглощается атмосферой и не отражается обратно к нашим глазам.\n",
      "\n",
      "Это рассеяние и поглощение солнечного света атмосферой Земли придает небу голубой цвет.\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=32000,\n",
    "    # eos_token_id=2,\n",
    "    pad_token_id=32000,\n",
    "    # pad_token_id=2,\n",
    "    # pad_token_id=2,\n",
    "    max_new_tokens=4000,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_orca_ru(instructions):\n",
    "    system = \"Вы помощник ИИ, который помогает людям находить информацию.\"\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system,\n",
    "            # \"content\": \"\",\n",
    "        },\n",
    "        # {\"role\": \"user\", \"content\": instruction},\n",
    "        *[\n",
    "            {\"role\": \"user\" if i % 2 == 0 else \"assistant\", \"content\": instruction}\n",
    "            for i, instruction in enumerate(instructions)\n",
    "        ],\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    # print(prompt)\n",
    "    # prompt = f\"<|im_start|>user\\n{instructions[0]}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    def generate(model, tokenizer, prompt, generation_config):\n",
    "        data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        # data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        data = {k: v.to(model.device) for k, v in data.items()}\n",
    "        output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "        output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "        # output_ids = output_ids[:]\n",
    "        # output = tokenizer.decode(output_ids, skip_special_tokens=False)\n",
    "        output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        return output.strip()\n",
    "\n",
    "    return generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "\n",
    "# print(generate_orca_ru(instructions=[\"Напиши алгоритм как погладить котика\"]))\n",
    "print(generate_orca_ru(instructions=[\"Почему трава зеленая?\"]))\n",
    "print(generate_orca_ru(instructions=[\"Почему небо голубое?\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saiga mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.24s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 1536,\n",
      "  \"no_repeat_ngram_size\": 15,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.2,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Почему трава зеленая?\n",
      "Вопрос о цвете травы зависит от многих факторов, таких как вид растения, условия окружающей среды, время года и т. д. Однако основной причиной того, почему трава обычно зеленая, является наличие в ней химических веществ, известных как хлорофиллы.\n",
      "\n",
      "Хлорофиллы являются пигментами, которые участвуют в процессе фотосинтеза - это процесс, благодаря которому растения получают энергию из солнечного света и превращают её в углеводы. Хлорофиллы способны поглощать световые волны, которые имеют длину 430-450 нм (синий цвет) и 670-680 нм (красный цвет). Эти волны используются для создания энергии, которая необходима для роста и развития растений.\n",
      "\n",
      "Хлорофиллы имеют особые структуры, которые позволяют им поглощать только эти две длины волн. Из-за этого они показывают себе зеленый цвет, когда освещены белым светом. Зеленый цвет - это результат того, что другие цвета, которые могут быть поглощены хлорофиллами, отражены или пропущены через них.\n",
      "\n",
      "Таким образом, трава обычно зеленая из-за наличия в ней хлорофиллов, которые играют ключевую роль в процессе фотосинтеза.\n",
      "\n",
      "==============================\n",
      "\n",
      "Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\n",
      "Таня была маленькой девочкой с золотистыми волосами и голубыми глазами. Она жила в небольшом городке, где все знали друг друга и всегда помогали друг другу. Таня была очень активной ребенком, она любила играть на улице с другими детьми, а особенно любила играть в футбол.\n",
      "\n",
      "Однажды вечером, когда Таня вернулась домой после игр на поле, она заметила, что у нее остался мяч. Мяч был старым и измученным, но для Тани он был самым ценным предметом в мире. Она провела целый день с ним, бросая его в воздух, пытаясь ударять по воротам и даже играя в \"подкидывание\" с друзьями.\n",
      "\n",
      "Но однажды, когда Таня играла в футбол со своим лучшим другом Петром, мяч внезапно разорвался на части. Таня была очень грустна, так как это был ее последний мяч, который она имела. Она попыталась найти новый мяч в магазинах, но ни один из них не подходил ей по цвету или форме.\n",
      "\n",
      "Вот когда Таня почти устала от поисков, она решила обратиться к своей маме за помощью. Мама Тани была очень умной женщиной, которая всегда помогала своим детям. Она сказала Тане, что может попробовать сделать мяч самостоятельно.\n",
      "\n",
      "Таня была радостна и сразу же начала искать материалы для создания нового мяча. Она нашла старую футбольную мяч, который еще был целым, но уже не подходит для игры. Она также нашла старую кофтовую ткань, которую можно использовать для обертки мяча.\n",
      "\n",
      "Таня приступила к работе над своим новым мячом. Сначала она вырезала из кофтовой ткани круглые кусочки и обвязывала их вокруг старого мяча. Затем она заполнила мяч старой кофтовой тканью, чтобы он стал более плотным и твердым. После этого она обвязала мяч еще одним слоем кофтовой ткани, чтобы он был более крепким и долговечным.\n",
      "\n",
      "Когда Таня закончила создание своего нового мяча, она была очень довольна результатом. Новый мяч был красивым и симпатичным, и он напоминал ее старый мяч. Таня снова стала играть в футбол со своими друзьями, и они были очень впечатлены ее усилиями.\n",
      "\n",
      "С тех пор, Таня стала известна как талантливая ремесленница и спортсменка. Ее новый мяч стал символом ее упорства и творчества. И каждый раз, когда она играла в футбол, она помнила о том, как она создала свой мяч и как много времени она потратила на это.\n",
      "\n",
      "Таня стала настолько хорошей в футболе, что даже получила шанс играть в профессиональном клубе. А ее мяч стал знаковым предметом для многих игроков и фанатов футбола. И все это благодаря тому, что она смогла превратить обычный старый мяч в красивое и симпатичное существо.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE,\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\"role\": \"bot\", \"content\": message})\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "generation_config.max_new_tokens = 4000\n",
    "\n",
    "inputs = [\n",
    "    \"Почему трава зеленая?\",\n",
    "    \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\",\n",
    "]\n",
    "for inp in inputs:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    print(inp)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Напишите интересный пост в блоге о недавней поездке на Гавайи, рассказывая о культурном опыте и достопримечательностях, которые обязательно нужно увидеть.\n",
      "Недавно я посетил Гавайи и был поражен разнообразием культур и достопримечательностей, которые меня ждали. От традиционных церемоний до современных достопримечательностей — каждый найдет что-то для себя.\n",
      "\n",
      "Одной из самых ярких культурных достопримечательностей Гавайев является традиционная культура гавайцев. Гавайцы известны своим теплым гостеприимством и любовью к природе. Они умеют строить красивые домики из тропических материалов и создавать уникальные формы для своих домов.\n",
      "\n",
      "Еще одна культурная достопримечательность Гавайев — традиционная архитектура. Гавайские архитекторы известны своим уникальным стилем, в котором сочетаются традиционные и современные элементы. Они используют местные материалы и строит дома, которые одновременно красивы и функциональны.\n",
      "\n",
      "Помимо традиционной культуры и архитектуры, Гавайи также предлагают множество других достопримечательностей. На Гавайях есть потрясающие пляжи с кристально чистой водой и белым песком. Они также известны своими традиционными танцами, такими как хулаху и хамелео.\n",
      "\n",
      "В целом, моя поездка на Гавайи была незабываемой. Я был поражен разнообразием культуры и достопримечательностей, которые меня ждали. Я настоятельно рекомендую посетить Гавайи, если у вас когда-нибудь будет возможность.\n",
      "==========\n",
      "==========\n",
      "Перепишите свой предыдущий ответ. Начните каждое предложение с буквы А.\n",
      "В заключение, Гавайи — это уникальное и увлекательное место, которое предлагает широкий спектр культурных и достопримечательностей, которые обязательно нужно увидеть. От традиционной культуры гавайцев до традиционной архитектуры и потрясающих пляжей — Гавайи — это место, которое обязательно нужно посетить. Я настоятельно рекомендую посетить Гавайи, если у вас когда-нибудь будет возможность.\n",
      "==========\n",
      "==========\n",
      "Подготовьте профессиональное электронное письмо с просьбой дать отзыв вашего руководителя о \"Ежеквартальном финансовом отчете\", который вы подготовили. Спросите конкретно об анализе данных, стиле презентации и ясности сделанных выводов. Сохраняйте электронное письмо коротким, пишите по существу.\n",
      "---\n",
      "\n",
      "Уважаемый [Имя руководителя],\n",
      "\n",
      "Я надеюсь это письмо благополучно дойдет до вас. Я пишу, чтобы попросить вашего одобрения и отзыв о \"Ежеквартальном финансовом отчете\", который мы недавно подготовили.\n",
      "\n",
      "Как вы знаете, в этом отчете представлен всесторонний анализ финансовой деятельности нашей компании за последние четыре месяца. Он включает в себя подробный анализ наших доходов и убытков, распределение средств и другие соответствующие финансовые данные.\n",
      "\n",
      "Я был бы очень признателен за ваши отзывы о следующих аспектах:\n",
      "\n",
      "1. Анализ данных: я хотел бы, чтобы вы оценили качество и точность данных, использованных в отчете. Вы должны убедиться, что данные собраны и представлены таким образом, чтобы их можно было легко понять и интерпретировать.\n",
      "\n",
      "2. Стиль презентации: ваша оценка стиля презентации, использованного в отчете, будет иметь решающее значение для эффективного донесения информации до наших заинтересованных сторон. Вы должны убедиться, что отчет хорошо структурирован и визуально привлекателен.\n",
      "\n",
      "3. Ясность выводов: я хотел бы, чтобы вы оценили ясность и краткость выводов, представленных в отчете. Вы должны убедиться, что выводы логически связаны и легко понятны.\n",
      "\n",
      "4. Общая структура отчета: ваша оценка структуры отчета, включая организацию, порядок и общую организацию отчета, будет иметь важное значение для обеспечения его эффективности и действенности.\n",
      "\n",
      "Я понимаю, что у вас может быть большой объем информации, которую нужно проанализировать и представить в ясной и лаконичной форме. Я был бы признателен, если бы вы могли предоставить мне ваши отзывы как можно скорее.\n",
      "\n",
      "Пожалуйста, дайте мне знать, если у вас есть какие-либо вопросы или опасения. Я с нетерпением жду вашего быстрого ответа.\n",
      "\n",
      "С наилучшими пожеланиями,\n",
      "\n",
      "[Ваше имя]\n",
      "\n",
      "---\n",
      "\n",
      "---\n",
      "\n",
      "[Имя руководителя]\n",
      "\n",
      "[Контактная информация]\n",
      "\n",
      "[Контактная информация]\n",
      "==========\n",
      "==========\n",
      "Найдите минутку, чтобы оценить и раскритиковать свой ответ.\n",
      "\n",
      "==========\n",
      "==========\n",
      "Представьте, что вы пишете сообщение в блоге, в котором сравниваете две популярные модели смартфонов. Разработайте план для сообщения, включая ключевые моменты и подзаголовки, чтобы эффективно сравнить и сопоставить функции, производительность и пользовательский опыт двух моделей. Пожалуйста, ответьте менее чем в 200 словах.\n",
      "---\n",
      "\n",
      "Вы можете найти мое сообщение в блоге [INST] о двух популярных моделях смартфонов, сравнивающих с точки зрения производительности, дизайна и пользовательского опыта.\n",
      "==========\n",
      "==========\n",
      "Возьмите свой предыдущий ответ и перефразируйте его как лимерик.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m conversation\u001b[38;5;241m.\u001b[39mappend(turn)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# prompt = conversation.get_prompt(tokenizer)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# output = generate(model, tokenizer, prompt, generation_config)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_orca_ru\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m replies\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     32\u001b[0m conversation\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mgenerate_orca_ru\u001b[0;34m(instructions)\u001b[0m\n\u001b[1;32m     43\u001b[0m     output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 39\u001b[0m, in \u001b[0;36mgenerate_orca_ru.<locals>.generate\u001b[0;34m(model, tokenizer, prompt, generation_config)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m data \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 39\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     40\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m output_ids[\u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]) :]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# output_ids = output_ids[:]\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# output = tokenizer.decode(output_ids, skip_special_tokens=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1718\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1702\u001b[0m         input_ids,\n\u001b[1;32m   1703\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1715\u001b[0m     )\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:2579\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2578\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2579\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2580\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1181\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1178\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1181\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1068\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1059\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1060\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         use_cache,\n\u001b[1;32m   1066\u001b[0m     )\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1068\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:796\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    795\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:410\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos}  \u001b[38;5;66;03m# Specific to RoPE models\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mupdate(key_states, value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx, cache_kwargs)\n\u001b[0;32m--> 410\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m value_states \u001b[38;5;241m=\u001b[39m repeat_kv(value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    413\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:273\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m    268\u001b[0m             down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n\u001b[0;32m--> 273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeat_kv\u001b[39m(hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, n_rep: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     batch, num_key_value_heads, slen, head_dim \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import shortuuid\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"dim/mt_bench_ru\")\n",
    "\n",
    "\n",
    "dataset = dataset[\"train\"].to_list()\n",
    "\n",
    "new_dataset = []\n",
    "model_id = \"tiny-llama-2T-open-orca-ru-10000-step\"\n",
    "base_save_path = f\"verbalist/evaluation/mt_bench/llm_judge/data/mt_bench/model_answer/{model_id}.jsonl\"\n",
    "\n",
    "for item in dataset:\n",
    "    replies = []\n",
    "    # conversation = Conversation()\n",
    "    # conversation = VerbalistOpenchatConversation()\n",
    "    # conversation = VerbalistConversation(bot_token_id=12435)\n",
    "    conversation = []\n",
    "    for turn in item[\"turns_ru\"]:\n",
    "        print(turn)\n",
    "        # conversation.add_user_message(turn)\n",
    "        conversation.append(turn)\n",
    "        # prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "        # output = generate(model, tokenizer, prompt, generation_config)\n",
    "        output = generate_orca_ru(instructions=conversation)\n",
    "        replies.append(output)\n",
    "        conversation.append(output)\n",
    "        # result = generate_orca_ru(instruction=turn)\n",
    "        # result = generate_orca_ru(instruction=turn)\n",
    "        print(output)\n",
    "        # conversation.add_bot_message(output)\n",
    "\n",
    "        print(\"=\" * 10)\n",
    "        print(\"=\" * 10)\n",
    "\n",
    "    new_dataset.append(\n",
    "        {\n",
    "            \"question_id\": item[\"question_id\"],\n",
    "            \"answer_id\": shortuuid.uuid(),\n",
    "            \"model_id\": model_id,\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"index\": 0,\n",
    "                    \"turns\": replies,\n",
    "                }\n",
    "            ],\n",
    "            \"tstamp\": time.time(),\n",
    "        }\n",
    "    )\n",
    "    # with open(base_save_path, \"a\") as f:\n",
    "    #     json.dump(new_dataset[-1], f, ensure_ascii=False)\n",
    "    #     f.write(\"\\n\")\n",
    "    # for item in new_dataset:\n",
    "    # break\n",
    "\n",
    "\n",
    "# with open(base_save_path, \"w\") as f:\n",
    "#     json.dump(\n",
    "#         new_dataset,\n",
    "#         fp=f,\n",
    "#         ensure_ascii=False,\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
