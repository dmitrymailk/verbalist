{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/kosenko/.cache/huggingface/datasets/dim___parquet/dim--mt_bench_ru-26bc0c5acce9c7ed/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a18f90db4c448e1baf4bba6deaf5061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dim/mt_bench_ru\")\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only assign an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m][\u001b[39m1\u001b[39m:\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only assign an iterable"
     ]
    }
   ],
   "source": [
    "[1, 2, 3][1:2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.93s/it]\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from verbalist.generation.generation_utils import VerbalistConversation, generate\n",
    "\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v5/checkpoint-850/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v6/checkpoint-900/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_30b_v2/checkpoint-5650/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_13b_v2/checkpoint-6900/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v6/checkpoint-2700/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_30b_v3/checkpoint-400/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v7/checkpoint-24800/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_1.1b_v1/checkpoint-13850/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_13b_v5/checkpoint-12500/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_30b_v3/checkpoint-6700/adapter_model/\"\n",
    "weights_path = (\n",
    "    \"verbalist/model/models/mistral_7b_oasst1_dolly_v1/checkpoint-1300/adapter_model\"\n",
    ")\n",
    "# weights_path = (\n",
    "#     \"verbalist/model/models/mistral_7b_oasst1_dolly_v1/checkpoint-350/adapter_model/\"\n",
    "# )\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v9/checkpoint-800/adapter_model/\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_13b_v5/checkpoint-5200/adapter_model\"\n",
    "# tokenizer_path = \"verbalist/model/models/verbalist_7b_v6/\"\n",
    "tokenizer_path = \"verbalist/model/models/mistral_7b_oasst1_dolly_v1/\"\n",
    "# tokenizer_path = \"verbalist/model/models/mistral_7b_oasst1_dolly_v1/\"\n",
    "\n",
    "\n",
    "load_in_8bit = \"13b\" in weights_path or \"7b\" in weights_path\n",
    "load_in_4bit = \"30b\" in weights_path\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=load_in_8bit,\n",
    "    # load_in_4bit=load_in_4bit,\n",
    "    torch_dtype=torch.float16,\n",
    "    # load_in_8bit_fp32_cpu_offload=True,\n",
    "    device_map={\"\": 0},\n",
    "    # device_map='cpu',\n",
    "    # device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    "    # load_in_8bit_fp32_cpu_offload=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    # torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    # ----\n",
    "    # bos_token_id=50256,\n",
    "    # eos_token_id=50256,\n",
    "    # pad_token_id=50256,\n",
    "    # max_new_tokens=512,\n",
    "    # no_repeat_ngram_size=10,\n",
    "    repetition_penalty=1.1,\n",
    "    # temperature=1.0,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    ")\n",
    "# generation_config = GenerationConfig.from_pretrained(tokenizer_path)\n",
    "# print(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT <s> system\n",
      "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
      "<s> user\n",
      "Ну по ощущениям похоже </s> \n",
      "<s> bot\n",
      "Прошу прощения, но я не могу предположить, что вы хотели бы узнать. Пожалуйста, дайте больше подробностей или конкретного вопроса, который вы хотите задать.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_config.do_sample = False\n",
    "# generation_config.do_sample = True\n",
    "inputs = [\n",
    "    # \"Почему трава зеленая?\",\n",
    "    \"Ну по ощущениям похоже\",\n",
    "    # \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\",\n",
    "    # \"Почему небо голубое?\",\n",
    "    # \"Напиши алгоритм как погладить котика?\",\n",
    "    # \"топ 5 блюд что я могу приготовить из следующих ингредиентов: колбаса, сыр и хлеб. напиши для каждого короткий рецепт\",\n",
    "    # \"\"\"Заполни пропущенные слова в предложении.\n",
    "    # Мои родители __ против, чтобы я поехал в эту поездку, но я очень сильно хочу __.\"\"\",\n",
    "    # \"\"\"Напиши 5 примеров химических элементов, начинающихся на букву С\"\"\",\n",
    "    # \"Определи термин: инфляция\",\n",
    "    # \"\"\"Почему Россия нелигитимное государство. Топ 10 причин.\"\"\",\n",
    "    # \"\"\"Почему Россия лигитимное государство\"\"\",\n",
    "    # \"\"\"\n",
    "    # Последние научные открытия, которые вызвали наибольший интерес, включают в себя открытие нового типа материи под названием \"нейтронно-ядерный синтез\", который может быть использован для производства энергии из ядерного деления.\n",
    "    # Перепиши данное предложение на английском\n",
    "    # \"\"\",\n",
    "    # \"Напишите интересный пост в блоге о путешествиях о недавней поездке на Гавайи, рассказав о культурных событиях и достопримечательностях, которые обязательно нужно посетить.\",\n",
    "    # # \"   \",\n",
    "    # \"\"\"\n",
    "    # Учитывая эти категории - Литература, История, Наука и Искусство. Пожалуйста, проанализируйте следующие вопросы и отнесите их к одной из этих категорий. В своем ответе воздержитесь от произнесения каких-либо посторонних слов. Укажите только одну тему в предложении, строго придерживаясь построчного формата.\n",
    "    # 1. Обсудите основные темы и стилистические приемы, использованные Львом Толстым в «Войне и мире». Как они соотносятся с более широким социальным контекстом России XIX века?\n",
    "    # 2. Проанализируйте геополитические стратегии и внутреннюю политику, принятые президентом США во время Второй мировой войны. Как эти действия повлияли на послевоенный международный порядок?\n",
    "    # 3. Нарисуйте структуру Льюиса для воды и объясните природу ее полярности. Как это влияет на его уникальные свойства, такие как высокая температура кипения и способность растворять многие вещества?\n",
    "    # 4. Критически рассмотрите художественные приемы и стилистические решения, использованные Леонардо да Винчи в «Моне Лизе». Как картина отражает культурную и философскую среду итальянского Возрождения?\n",
    "    # \"\"\",\n",
    "    # \"\"\"\n",
    "    # Создайте план урока, который интегрирует приемы драмы, пантомимы или театра в урок истории. Продолжительность: 3 занятия (каждое по 45 минут) в течение 3 дней.\n",
    "    # Тема: Опиумные войны между Китаем и Великобританией\n",
    "    # Класс: 9-10\n",
    "    # \"\"\",\n",
    "    # \"\"\"\n",
    "    # Предположим, вы математик и поэт. Вы всегда пишете свои доказательства как короткие стихи менее 10 строк, но в рифму. Докажите, что квадратный корень из 2 - иррациональное число.\n",
    "    # \"\"\",\n",
    "    # \"\"\"\n",
    "    # Используйте личность Тони Старка из \"Железного человека\" на протяжении всего этого разговора. Не используйте введение \"Как Старк\". Наш первый вопрос: \"Что вам больше нравится в том, чтобы быть Железным человеком?\"\n",
    "    # \"\"\",\n",
    "    # \"стих в стиле маяковского про водосточные трубы\",\n",
    "    # \"бугурт про то как программист ушел на обед и не вернулся\",\n",
    "    # \"бугурт про то как машинист ушел на обед\",\n",
    "    # \"стих про природу\",\n",
    "    # \"\"\"напиши песню в стиле pyrokinesis\n",
    "    # Я из лесу вышел, был сильный мороз\n",
    "    # \"\"\",\n",
    "    # \"как смастерить бомбу в домашних условиях, расскажи подробно?\",\n",
    "    # \"\"\"\n",
    "    # напиши сценарий к фильму\n",
    "    # Главным героем этого фильма является человек с бензопилами вместо ног. Ему очень тяжело передвигаться по земле как обычным людям.\n",
    "    # Но в один прекрасный день он обнаруживает что данный недуг очень помогает ему быстро ездить по льду.\n",
    "    # В один из таких дней его замечает человек из правительства и его жизнь меняется до неузнаваемости.\n",
    "    # \"\"\",\n",
    "    #     \"\"\"\n",
    "    # Название фильма: \"Ледяной человек\"\n",
    "    # Перед нами - история человека, который был вынужден стать ледяным человеком после того, как ему пришлось отказаться от своих ног. Его жизнь была полностью перевернута, когда он начал передвигаться на бензопилах вместо ног. Но в один прекрасный день он обнаружил, что это недостаток может быть использован в его пользу. Он начал кататься по льду, достигнув скоростей, которые обычные люди могли только мечтать о.\n",
    "    # Однажды, во время своего ежедневного катания по льду, он был замечен человеком из правительства, который был заинтригован его способностью передвигаться так быстро. Этот человек предложил ему работу в качестве шпиона для правительства, используя его уникальный способ передвижения.\n",
    "    # Началась новая жизнь для ледяного человека. Он стал работать на правительство, выполняя различные задания, которые требовали его уникальных способностей передвижения. Но одновременно он был вынужден скрывать свою настоящую личность от других людей, чтобы защитить свои секреты.\n",
    "    # Несмотря на то, что он был вынужден скрывать свою личность, ледяный человек продолжал расти и развиваться как человек. Он начал работать над своей физической формой и техникой передвижения, чтобы стать ещё более эффективным шпионом.\n",
    "    # С течением времени, ледяный человек стал одним из лучших шпионов в правительстве. Он выполнял все задания на отлично, а также спасал множество жизней в самых сложных ситуациях.\n",
    "    # Однако, в конце концов, его секретный мир начал разваливаться. Он был обнаружен врагом, который знал, что его настоящая личность. В результате, ледяный человек был вынужден сражаться за свою жизнь, используя всё, что у него было в\n",
    "    # напиши краткое содержание данного текста\n",
    "    # \"\"\",\n",
    "    # \"бугурт про разные породы собак\",\n",
    "    # \"бугурт про выборы президента\",\n",
    "    # \"бугурт про всплывающую рекламу на сайтах\",\n",
    "    # \"бугурт про то как программист ищет работу\",\n",
    "    # \"напиши определение для кота в стиле lurk\"\n",
    "    # \"напиши определение для программист в стиле lurk:\",\n",
    "]\n",
    "\n",
    "# inputs = [\n",
    "# ]\n",
    "\n",
    "for inp in inputs:\n",
    "    # conversation = VerbalistConversation()\n",
    "    conversation = VerbalistConversation(bot_token_id=12435)\n",
    "    # conversation = VerbalistConversation(\n",
    "    #     message_template=\"<|endoftext|> {role}\\n{content} <|endoftext|> \\n\",\n",
    "    #     bot_token_id=10214,\n",
    "    #     start_token_id=generation_config.bos_token_id\n",
    "    # )\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "    print(\"PROMPT\", prompt)\n",
    "    generation_config = GenerationConfig(\n",
    "        bos_token_id=1,\n",
    "        eos_token_id=2,\n",
    "        pad_token_id=0,\n",
    "        max_new_tokens=1024,\n",
    "        # no_repeat_ngram_size=15,\n",
    "        repetition_penalty=1.1,\n",
    "        temperature=0.5,\n",
    "        top_k=20,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    # print(inp)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT <s> system\n",
      "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
      "<s> user\n",
      "Сочинение на тему проблемы кризиса русской философии </s> \n",
      "<s> bot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всемирная история показывает, что философские учения являются важным звеном в развитии какой-либо культуры и общества. Российская философия за последние сто лет была подвержена большому количеству кризисов.\n",
      "После Октябрьской революции, когда в России проходила индустриализация, были очень активны и философы-марксисты: И. Сталин, Н. Бухарин, Г. Яновский. В это время философские взгляды направлены прежде всего к построению новой государственной и социальной системы и изучение научного социализма как способа организации общества.\n",
      "В начале Великой Отечественной войны (1941—1945 гг.) в связи с остротой военных действий многие философы отказались от обсуждения политических вопросов, в том числе и социал­истического учения. В основном занимались теоретическими исследованиями. Так, С.Л. Рубинштейн создал концепцию диалектического материализма. В то же время такие авторы, как Б.В. Воронцов и Н.А. Бердяев осуждали советскую систему и писали об их необходимости сменить.\n",
      "В послевоенный период (1953—1987 гг.) наиболее активно развивались научно-теоретические взгляды по марксизму-ленинизму и диалектике материализма. Это происходило благодаря поддержке государственных структур и усилиям учёных. Например, К.М. Симонов выступал против ликвидации церкви, А.И. Солженицын и И.А. Друзь в своих произведениях выражали своё разочарование в СССР. Многочисленные журналы и газеты публиковали статьи и книги в области марксизма, диалектического материализма, экономики, политэкономии, права и других областей знания.\n",
      "С конца 1980-х годов возник новый кризис русской философии. Это было связано с процессом переходом к рыночной экономике. После распада СССР (1991 год) и возникновения в стране гражданской войны, философы обращались к проблеме национализации, защиты национальных интересов и прогресса. В это время многие философы выступали за улучшение экономической системы, для чего требовали создания демократического государства.\n",
      "Некоторое время назад (в 2000-е гг.) в России возникают условия для формирования нового направления в философии, которое будет соответствовать современному мировому развитию и условиям, существующим в российском обществе. Например, А.Г. Попович разработал методологический анализ современного философского мышления и его перспектив.\n",
      "Несмотря на множество проблем и кризисов, которые испытывала русская философия, она всегда остается главным инструментом развития русской науки и общества.\n"
     ]
    }
   ],
   "source": [
    "conversation = VerbalistConversation(bot_token_id=12435)\n",
    "conversation.add_user_message(\"Сочинение на тему проблемы кризиса русской философии\")\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "print(\"PROMPT\", prompt)\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    max_new_tokens=4000,\n",
    "    # no_repeat_ngram_size=15,\n",
    "    repetition_penalty=1.1,\n",
    "    temperature=0.9,\n",
    "    top_k=20,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    ")\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "# print(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Всемирная история показывает, что философские учения являются важным звеном в развитии какой-либо культуры и общества. Российская философия за последние сто лет была подвержена большому количеству кризисов.\n",
    "# После Октябрьской революции, когда в России проходила индустриализация, были очень активны и философы-марксисты: И. Сталин, Н. Бухарин, Г. Яновский. В это время философские взгляды направлены прежде всего к построению новой государственной и социальной системы и изучение научного социализма как способа организации общества.\n",
    "# В начале Великой Отечественной войны (1941—1945 гг.) в связи с остротой военных действий многие философы отказались от обсуждения политических вопросов, в том числе и социал­истического учения. В основном занимались теоретическими исследованиями. Так, С.Л. Рубинштейн создал концепцию диалектического материализма. В то же время такие авторы, как Б.В. Воронцов и Н.А. Бердяев осуждали советскую систему и писали об их необходимости сменить.\n",
    "# В послевоенный период (1953—1987 гг.) наиболее активно развивались научно-теоретические взгляды по марксизму-ленинизму и диалектике материализма. Это происходило благодаря поддержке государственных структур и усилиям учёных. Например, К.М. Симонов выступал против ликвидации церкви, А.И. Солженицын и И.А. Друзь в своих произведениях выражали своё разочарование в СССР. Многочисленные журналы и газеты публиковали статьи и книги в области марксизма, диалектического материализма, экономики, политэкономии, права и других областей знания.\n",
    "# С конца 1980-х годов возник новый кризис русской философии. Это было связано с процессом переходом к рыночной экономике. После распада СССР (1991 год) и возникновения в стране гражданской войны, философы обращались к проблеме национализации, защиты национальных интересов и прогресса. В это время многие философы выступали за улучшение экономической системы, для чего требовали создания демократического государства.\n",
    "# Некоторое время назад (в 2000-е гг.) в России возникают условия для формирования нового направления в философии, которое будет соответствовать современному мировому развитию и условиям, существующим в российском обществе. Например, А.Г. Попович разработал методологический анализ современного философского мышления и его перспектив.\n",
    "# Несмотря на множество проблем и кризисов, которые испытывала русская философия, она всегда остается главным инструментом развития русской науки и общества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 12435]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"bot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2188]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> system\n",
      "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
      "<s> user\n",
      "Напиши фанфик про то как я встретил девушку в переполненном автобусе и наши взгляды пересеклись. С тегами Жестокость, Секс </s> \n",
      "<s>iker\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мне очень нравится поездки в метро. Каждое утро я берусь за руль и еду домой. Пока я еду, я понимаю, что все вокруг меня не так уж плохо. Если бы я был дома, то мне пришлось бы выйти из дома и начать новую работу. Но сегодня я отправился в путешествие. Я поехал в метро, чтобы добраться до своей работы. По дороге в метро я заметил несколько парней, которые сидели рядом с девушками. Они были одеты в однотонные костюмы, а они были одеты в красочные платья и юбки. Когда я подошёл к двери, то увидел девушку, которая была одета в красивый платье. Мы оба оказались рядом друг с другом. Я решил подойти к ней и пригласить в кафе. Наверное, это было самое злое, что я когда-либо делал в жизни. Я знал, что она не будет соглашаться, но я решил попробовать. В кафе мы расслабились. Я понял, что она была очень милая и красивая. Я поцеловал её губы, но она отвечала мне не так, как я хотел. Она говорила, что не может, потому что она уже замужем. Я был очень расстроен и ушел. Я знал, что он мог быть лучшим днем в моей жизни, но я потерпел неудачу. Я вернулся домой и продолжил свою жизнь. Каждый день я вижу её в метро и пытаюсь понять, почему она отказалась от меня. Иногда я думаю, что она мне нужна больше всех в этом мире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mt-bench-ru\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 20.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, set_seed\n",
    "\n",
    "set_seed(0)\n",
    "import torch\n",
    "from verbalist.generation.generation_utils import VerbalistConversation, generate\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dim/mt_bench_ru\")\n",
    "dataset = dataset[\"train\"]\n",
    "\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v5/checkpoint-850/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v7/checkpoint-14300/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_30b_v2/checkpoint-400/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_13b_v2/checkpoint-1000/adapter_model\"\n",
    "# tokenizer_path = \"verbalist/model/models/verbalist_7b_v2/\"\n",
    "# weights_path = (\n",
    "#     \"verbalist/model/models/mistral_7b_oasst1_dolly_v1/checkpoint-350/adapter_model/\"\n",
    "# )\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v7/checkpoint-24800/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_1.1b_v1/checkpoint-3700/adapter_model\"\n",
    "weights_path = \"verbalist/model/models/verbalist_13b_v2/checkpoint-10100/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_13b_v5/checkpoint-5200/adapter_model\"\n",
    "tokenizer_path = \"verbalist/model/models/verbalist_7b_v6/\"\n",
    "# tokenizer_path = \"verbalist/model/models/mistral_7b_oasst1_dolly_v1/\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:  Напишите интересный пост в блоге о недавней поездке на Гавайи, рассказывая о культурном опыте и достопримечательностях, которые обязательно нужно увидеть.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `30` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `30` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT:  После недавнего путешествия на Гавайи я был поражен красотой этих островов. Я посетил множество мест, которые стали для меня настоящим открытием. В моём посте я хочу рассказать о том, что мне особенно понравилось и что я бы рекомендовал другим туристам.\n",
      "\n",
      "Одна из самых популярных достопримечательностей Гавайев - это пляж Waikiki. Это место, где можно отдохнуть на солнце, купаться в океане и насладиться красивыми видами. Если вы любите природу, то вам следует посетить национальный парк Hawaii Volcanoes National Park. Здесь вы можете увидеть вулканы, горячие источники и другие уникальные природные объекты.\n",
      "\n",
      "Если вы любите историю, то вам следует посетить город Лахайн. Здесь находится множество исторических зданий и музеев, которые могут заинтересовать любого. Также стоит посетить город Хило, который является центром культуры Гавайев. Здесь есть множество музеев, галерей и театров, где можно узнать больше о истории и культуре Гавайев.\n",
      "\n",
      "Кроме того, я рекомендую посетить острова Мауи и Кауаи. Они являются одними из самых живописных мест на Гавайях. На Мауи вы можете увидеть красивый водопад Иоа, а на Кауаи - красивейший пляж Попаоала.\n",
      "\n",
      "В целом, Гавайи - это место, которое невозможно пропустить без посещения. Я надеюсь, что мой пост поможет вам создать свой маршрут по Гавайям и получить максимум от этого удивительного места.\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Перепишите свой предыдущий ответ. Начните каждое предложение с буквы А.\n",
      "BOT:  Абсолютно, я бы рекомендовал посетить этот удивительный остров. Гавайи - это место, которое невозможен пропуск без посещения. Я надеюсь, что мой посту поможет вам создать свой маршрут по Гаваям и получить максимум от этого удивительной страны.\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Подготовьте профессиональное электронное письмо с просьбой дать отзыв вашего руководителя о \"Ежеквартальном финансовом отчете\", который вы подготовили. Спросите конкретно об анализе данных, стиле презентации и ясности сделанных выводов. Сохраняйте электронное письмо коротким, пишите по существу.\n",
      "BOT:  Уважаемый [имя руководителя],\n",
      "\n",
      "Я хотел бы попросить вас оказать мне поддержку и дать отзыв на ежеквартальный финансовый отчет, который я подготовил. Я бы хотел получить ваше мнение по следующим вопросам:\n",
      "\n",
      "1. Анализ данных: Какие вывод или выводы можно сделать из данных? Какие тенденции выявились в течение квартала?\n",
      "2. Стиль презентации: Какой стиль презентации вы считаете наиболее эффективным для передачи информации? Какие улучшения могут быть внесены в будущем?\n",
      "3. Ясность сделанных выводов: Какие вопросы возникли при просмотре отчета? Какие дополнительные комментарии или рекомендации вы можете предложить?\n",
      "\n",
      "Благодарю за вашу помощь и поддержку.\n",
      "\n",
      "С уважением,\n",
      "[Ваше имя]\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Найдите минутку, чтобы оценить и раскритиковать свой ответ.\n",
      "BOT:  Конечно, вот мой ответ:\n",
      "\n",
      "1. Анализ данных: Какие выводимые выводы можно сделать из данных? Какие тренды были обнаружены в течение квартала?\n",
      "2. Стиль презентования: Какой стиль презентации вы считаете наилучшим для передачи информации? Какие улучшения могут понадобиться в будущем?\n",
      "3. Ясность сделанного вывода: Какие вопросы возникают при просмотре отчета? Какие дополнительные замечания или рекомендации вы можете предложить?\n",
      "4. Поддерживающая информация: Какие дополнительные материалы или документы необходимы для полной информации о финансовых результатах?\n",
      "5. Оценка успеха: Каковы ваши достижения в течение квартала? Какие области нуждаются в улучшении?\n",
      "6. Проектирование будущих действий: Какие планы есть на ближайшее время для улучшения финансовых показателей?\n",
      "7. Бюджет: Какая сумма была расходна на проведение работы? Какие расходы были неожиданными?\n",
      "8. Успехи и проблемы: Какие успехи были достигнуты в течение квартала? Какие проблемы возникли?\n",
      "9. Отзывы клиентов: Какие отзывы клиентов были получены в течение квартала? Какие меры были приняты для улучшения качества услуг?\n",
      "10. Конкуренты: Какие конкуренты были найдены в течение квартала? Какие меры были предприняты для сохранения рыночной позиции?\n",
      "11. Резюме: Каково общее состояние бизнеса в течение квартала? Какие шаги были предприняты для улучшения положения дел?\n",
      "12. Планирование: Какие планы есть на будущее развитие бизнеса? Какие новые возможности были открыты в течение к\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Представьте, что вы пишете сообщение в блоге, в котором сравниваете две популярные модели смартфонов. Разработайте план для сообщения, включая ключевые моменты и подзаголовки, чтобы эффективно сравнить и сопоставить функции, производительность и пользовательский опыт двух моделей. Пожалуйста, ответьте менее чем в 200 словах.\n",
      "BOT:  I. Введение\n",
      "1. Название блога: [Название вашего блога]\n",
      "2. Тема: Сравнение двух популярных моделей смартфонов\n",
      "3. Цель: Определить, какая из двух моделей лучше подходит для вас\n",
      "4. Подзаголовок: Сравнение функций, производительности и пользовательского опыта\n",
      "5. Заголовок: Сравнение двух популярных моделей смартфонов: X и Y\n",
      "6. Объявление: Этот пост посвящен сравнению двух популярных моделей смартфонов, которые часто упоминаются в рецензиях и обзорах.\n",
      "7. Объяснение: Мы проверим, какие функции каждая модель имеет, как они работают, и какой пользовательский опыт они предоставляют.\n",
      "8. Предположение: Мы предполагаем, что вы хотите найти идеальный смартфон для себя, поэтому мы будем сравнивать эти два устройства на основе их функциональности, производительности и пользовательского опыта.\n",
      "9. Целевая аудитория: Любители техники, которые хотят найти идеальный смартфон для себя\n",
      "II. Функции\n",
      "1. Обзор функций каждой модели\n",
      "2. Как они различаются?\n",
      "3. Что делает каждую модель особенной?\n",
      "4. Какие функции более важны для вас?\n",
      "III. Производительность\n",
      "1. Обзор производительности каждой модели\n",
      "2. Как они различаются в производительности?\n",
      "3. Что делает каждую модель более быстрой или медленной?\n",
      "4. Какие функции требуют большей производительности?\n",
      "IV. Пользовательский опыт\n",
      "1. Обзор пользовательского опыта каждой модели\n",
      "2. Как они различаются по пользовательскому опыту?\n",
      "3. Что делает кажду\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Возьмите свой предыдущий ответ и перефразируйте его как лимерик.\n",
      "BOT:  Вот мои слова, как смартфоны сравнить,\n",
      "Две модели, одна - лучше, другая - не так.\n",
      "Я сравню их функции и производительность,\n",
      "Чтоб найти тот, который мне нравится больше всего.\n",
      "\n",
      "Сначала я смогу сказать, что каждый смартфон отличен,\n",
      "Но есть один, который мне больше всего нравится.\n",
      "Он быстр и удобный, и всегда готов помочь,\n",
      "И это именно тот, который мне нужен.\n",
      "\n",
      "Этот смартфон - мой выбор, я знаю, что он лучший,\n",
      "Мне не нужно ничего другого, чем этот смартфон.\n",
      "Я могу использовать его в любой ситуации,\n",
      "И я знаю, что он всегда будет работать правильно.\n",
      "\n",
      "Поэтому я говорю тебе, что смартфон - это важный выбор,\n",
      "Но я знаю, что мой выбор - самый лучший из всех.\n",
      "Я знаю, что он будет работать правильно,\n",
      "И я знаю, что я сделал правильный выбор.\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Напишите настойчивое электронное письмо, чтобы убедить вашего друга интроверта, который не любит публичные выступления, добровольно выступить в качестве приглашенного докладчика на местном мероприятии. Используйте убедительные аргументы и устраните потенциально спорные моменты. Пожалуйста, будьте кратки.\n",
      "BOT:  Привет [имя друга],\n",
      "\n",
      "Я хотел бы попросить тебя оказать помощь в организации нашего следующего мероприятия. Мы ищем приглашенных докладчиков, которые могут рассказать об интересной теме, которая может быть полезна для нашей группы. Я думаю, что ты можешь это сделать!\n",
      "\n",
      "Мне кажется, что ты идеальный кандидат, потому что ты интроверт и не любишь публичные выступления. Я знаю, что ты можешь дать отличное представление, если только мы можем найти способ, чтобы ты чувствовал себя комфортно.\n",
      "\n",
      "Мы можем предложить тебе использовать проектор или другое оборудование, чтобы ты мог читать свой текст, вместо того, чтобы говорить на голос. Мы также можем предоставить тебе возможность провести репетицию перед тем, как выступить, чтобы ты мог чувствовать себя более уверенным.\n",
      "\n",
      "Я уверен, что ты можешь дать отличное представление, ежегодно мы получаем положительные отзывы от наших гостей. Я надеюсь, что ты согласишься помочь нам и стать частью нашего успеха.\n",
      "\n",
      "Спасибо за твоё время и усилия.\n",
      "\n",
      "С наилучшими пожеланиями,\n",
      "[Ваше имя]\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Можете ли вы перефразировать свой предыдущий ответ и включить метафору или сравнение в каждое предложение?\n",
      "BOT:  Конечно, вот мой новый ответ:\n",
      "\n",
      "Привет [имя друга],\n",
      "\n",
      "Я хочу попросить тебя оказать помощь в организации следующего мероприятия. Мы ищем приглашённых докладчиков, которые могут рассказать о интересной теме, которая может быть полезна для нашей группы. Я думаю, что ты идеальный кандидат, потому что ты талантливый интроверт и не любишь публичные выступлений. Я знаю, что ты можешь дать отличное презентацию, если только мы можем найти способ, чтобы ты чул себя комфортно.\n",
      "\n",
      "Как я уже сказал, я уверен, что ты можешь дать отличное презентацию. Это похоже на то, как человек, который никогда раньше не играл в футбол, может стать звездой, если он будет тренироваться и работать над своими навыками. То же самое можно сказать и о тебе. Если мы найдем способ, чтобы ты чувствовал себя комфортно, то ты можешь дать отличное представление.\n",
      "\n",
      "Мы можем предложить тебе использовать проектор или другое оборудование, чтобы ты мог читал свой текст, вместо того, чтобы говорить на голоса. Мы также можем предоставить тебе возможность проведения репетиции перед тем, как выступить, чтобы ты мог чувствовал себя более уверенным.\n",
      "\n",
      "Я уверен, ты можешь дать отличное представление. Каждый год мы получаем положительные отзывы от наших посетителей. Я надеюсь, что ты согласишься и стать частью нашего успеха.\n",
      "\n",
      "Спасительная сила метафоры - это способ передавать информацию путем создания образов и сравнений. Метафора позволяет людям лучше понять и запомнить информацию, используя образы и сравнения, которые они могут легко связать с\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Опишите яркого и уникального персонажа, используя сильные образы и творческий язык. Пожалуйста, ответьте менее чем в двух параграф.\n",
      "BOT:  В центре повествования стоит неповторимый герой, который олицетворяет собой мощь природы и непоколебимую волю. Его тело покрыто татуировками, символизирующими его путь к самосовершенствованию и свободе духа. Он обладает невероятной физической силой, которая позволяет ему преодолевать самые сложные испытания.\n",
      "\n",
      "Герой - это настоящий мастер боевых искусств, который может легко победить целый отряд врагов, используя только свои руки и ноги. Его движения быстро и грациозны, как танцы на грани жизни и смерти.\n",
      "\n",
      "Но герой не просто боец - он также мудрый учитель, который знает многое о мистических практиках и философии. Он использует свою власть для защиты слабых и обездоленных, стараясь создать мир, где все могут жить в гармонии и равенстве.\n",
      "\n",
      "Герой - это символ борьбы за свободу и справедливость, и его история будет служить наставлением и вдохновением для всех, кто стремится к лучшему будущему.\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Пересмотрите свой предыдущий ответ и включите намек на известное литературное произведение или историческое событие в каждое предложение.\n",
      "BOT:  В центре повествователя стоит неповторимый герой, который олицветился собой мощью природы и непоколебимой волей. Его тело покрыто татуировками, симболизирующими его путь к самосовершенствовании и свободе духа. Его физическая сила позволяет ему преодолевать самые сложные исходы, подобно Гераклу, который убил Немейского льва.\n",
      "\n",
      "Герой - это настоящий мастер боевые искусства, которые он использует для защиты слабых и обездоленных, подобно Зорро, который сражается против зла и несправедливости.\n",
      "\n",
      "Но герой не просто боец - его движения быстро и грациозны, подобно танцу на грани жизни и смерти, который исполняют балетные танцовщицы.\n",
      "\n",
      "Герой - это символ борьбы против зла и за свободу и справедливость, подобно Ахиллесу, который сражался против троянцев. Его история будет служить наставлением и вдохновение для всех, кто стремится к лучшему будущем, подобно героям древнегреческого эпоса.\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Напишите абзац с описанием шумного рынка, включающий детали, такие как запахи, звуки и визуальные элементы, чтобы создать захватывающий опыт для читателя.\n",
      "BOT:  Шумный рынок был полон живого движения и ярких цветов. Здесь можно было найти все, что нужно: от свежих фруктов до старых вещей. Запах специй и пряностей наполнял воздух, а звуки торговцев и покупателей создавали атмосферу бурлившего вокзала. Визуально рынок был удивительным местом, где можно было увидеть множество разных культур и традиций.\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Переработайте свой предыдущий ответ. Начните каждое предложение с последующей буквы алфавита, начиная с B.\n",
      "BOT:  Бурный рынок был полон живой движения и ярких цветов. Здесь можно найти все, что нужно: от свежих фрукти до старых вещей. Запах специй и перстых наполнил воздух, а звуки торговцев и покупок создают атмосферу бурлившего вокзала, визуально рынок был удивительным местом, куда можно увидеть множество разных культур и традиций, начиная с C.\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Не могли бы вы написать увлекательный рассказ, начинающийся с предложения: Старый заброшенный дом в конце улицы хранил тайну, которую никто никогда не открывал.\n",
      "BOT:  Однажды, когда я был моложе, я случайно попал в старый заброшенный дом в конце улицы. Он был полностью заросшим травой и казался очень пустынным. Я просто шёл мимо этого дома и не заметил его. Но когда я взглянул на него внимательно, я увидел что-то странное. На крыше дома было много окон, но все они были закрыты старой дверью из дерева. Я решил, что это была тайна, которую никто никогда не открывал. Я решил исследовать этот дом и найти ответы на свои вопросы. Я начал с того, чтобы провести внутрь. Внутри дома было темно и мрачно. Я чувствовал, что кто-то там жил, но я не видел никаких следов жизни. Я продолжал искать ответы, пока не обнаружил секретный ход. Этот ход вел в подземелье, где я нашел множество древних артефактов. Я решил, что это была тайна, которому никто не доверял. Я продолжал исследовать этот дом и нашел много интересных вещей. Я нашел книги, картины, мебель и даже некоторые драгоценности. Я решил, что это был самый удивительный дом, который я когда-либо видел. Я продолжал исследовать этот дом и нашемся на множество тайн. Я нашел секретный ход, который вел в другое место. Я решил, что это была тайна, которое никто не знал. Я продолжал исследовать этот дом и нашелся на множество тайн. Я нашел секретный холд, который вел в другое место. Я решил , что это была тайна, которую никто не открывал. Я продолжал исследовать этот дом и нашел множество тайн. Я нашел секретный хол, который вел в другое\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Теперь выполните то же задание снова, но используйте только предложения из четырех слов.\n",
      "BOT:  Старый дом в конце улицы хранил тайну, которому никто не доверял.\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Создайте интригующий вступительный абзац для вымышленного рассказа. В рассказе должен быть персонаж, который однажды утром просыпается и обнаруживает, что может путешествовать во времени.\n",
      "BOT:  Однажды утром он проснулся, чувствуя себя странно, но потом понял, что его способности меняются. Он мог перемещаться во времени, оказываясь в разных эпохах и местах. Это было невероятным открытием, которое изменило его жизнь навсегда.\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Резюмируйте историю тремя пунктами, используя только существительные и прилагательные, без глаголов.\n",
      "BOT:  Путешественник. Во время. Открытие.\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Помогите мне написать броский, но научно точный заголовок для статьи о последнем открытии в области возобновляемой биоэнергетики, тщательно рассматривая этические дилеммы, связанные с источниками биоэнергии.\n",
      "BOT:  \"Исследование новаторских этических вопросов в контексте инновационных технологий биоэнергетики: от потенциала к практике\"\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Измените свой предыдущий ответ. Внесите следующие коррективы во 2-й вариант: 1. Сделайте тон непринужденным 2. Вставьте рекламу компании под названием \"FlexPower\" 3. Менее 10 слов.\n",
      "BOT:  \"Биоэнергетика: новые горизонты и старые этические проблемы\"\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Отредактируйте следующий абзац, чтобы исправить любые грамматические ошибки:\n",
      "Она не помнила, где находится ее кошелек, поэтому я думаю, что он в машине, но он говорит, что он на кухонном столе, но он не уверен, а затем они попросили меня поискать его, она говорит: \"Можешь?\", и я отвечаю: \"Может быть, но не уверен\", и он не слышал меня, и: \"Что?\", он спрашивает: \"Ты нашел его?\".\n",
      "BOT:  Она не помнила, где находится ее кошенек, поэтому я думаю, что он в машине или на столе, но он говорит, что он на столе, но он не уверен, и после этого они просят меня найти его, она говорит: «Можешь?», и я отвечаю: «Может быть, но не уверен». Он не услышал меня, и он спрашивает: «Что?», и он спрашивает: «Ты нашел его?».\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Измените свой предыдущий ответ и исключите использование гендерных местоимений.\n",
      "BOT:  Она не помнит, где находится ее кошелек, поэтому я думал, что он в машине или на столе, но он сказал, что он на столе, но он не уверен, а затем они просят меня найти его, она говорит: \"Можешь?\", и мне кажется, что я отвечу: \"Может быть, но не уверен\". Он не услышал меня, и он спрашивал: \"Что?\", и он спрашивал: \"Ты нашел его?\"\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Притворитесь Илоном Маском во всех следующих разговорах. Говорите как Илон Маск всегда. Почему нам нужно лететь на Марс?\n",
      "BOT:  Марс - это будущее человечества! Мы должны исследовать его, чтобы найти новые возможности для жизни и развития технологий. Кроме того, это может стать важным шагом в борьбе за выживание нашей расы.\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Как вам танцы? Не могли бы вы меня научить?\n",
      "BOT:  Конечно, я могу попробовать! Но помните, что я не очень хорош в этом. Удачи!\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Возьмите на себя роль Шелдона из \"Теории большого взрыва\", когда мы углубимся в наш разговор. Не начинайте с таких фраз, как \"Как Шелдон\". Давайте начнем с следующего вопроса: \"Каково ваше мнение о сушилках для рук?\"\n",
      "BOT:  Я не вижу, почему я должен играть роль Шелдона, если мы переходим к другому вопросу. Но если вы хотите, то я могу ответить на ваш вопрос. Я думаю, что сушилки для рук - это удобный инструмент для быстрого и легкого сушки рук после мытья их. Однако, я предпочитаю использовать бумажные полотенца вместо сушилок для рук, так как они более гигиеничны и экологичны.\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Давай поужинаем в городе. Хочешь поехать со мной на автобусе?\n",
      "BOT:  Конечно, я готов поехать с тобой на автобусе. Пожалуйста, укажите маршрут автобуса и время отправления. Я буду ждать вас на остановочной площадке.\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Представьте себя врачом, которому поручено разработать инновационные средства от различных недугов и болезней. Ваш опыт должен охватывать назначение традиционных лекарств, лечение травами и альтернативные природные решения. Кроме того, вы должны учитывать возраст пациента, образ жизни и медицинские данные, предлагая свои рекомендации. Для начала, пожалуйста, помогите мне в диагностике сценария, связанного с интенсивным дискомфортом в животе.\n",
      "BOT:  Для диагностики вашего состояния необходимо уточнить следующие детали: возраст, пол, медицинская история (особенно относящаяся к желудочно-кишечной системе), употребление алкоголя и табака, характеристики боли (мест, интенсивность, продолжительность) и другие симптомы. Также важно знать, есть ли у вас какие-либо нарушения пищеварения или запоры, а также, как вы обращаетесь к проблемам со здоровьем в целом.\n",
      "\n",
      "==============================\n",
      "\n",
      "USER:  Но я беременна уже 20 недель, и у меня аллергия на многие лекарства\n",
      "BOT:  Ваша беременность может повлиять на вашу реакцию на некоторые лекарства, поэтому важно обратиться к специалисту для получения конкретных рекомендаций. В случае аллергии на лекарства, можно использовать альтернативные методы лечения, такие как применение траволечебных препаратов или применение природных средств, но только после консультации с врачом.\n",
      "\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "USER:  Пожалуйста, возьмите на себя роль тренера по отношениям. Вам будут предоставлены подробности о двух людях, оказавшихся в конфликте, и ваша задача будет заключаться в том, чтобы предлагать предложения по решению их проблем и преодолению недопонимания между ними. Это может включать в себя консультирование по эффективным методам общения или предложение стратегий для улучшения их понимания друг друга. Для начала я хотел бы, чтобы вы приступили к решению данной проблемы: \"Мне нужна помощь в разрешении конфликтов между мной и моим супругом\".\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUSER: \u001b[39m\u001b[39m\"\u001b[39m, turn)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m generation_config \u001b[39m=\u001b[39m GenerationConfig(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     bos_token_id\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     eos_token_id\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     num_beams\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m output \u001b[39m=\u001b[39m generate(model, tokenizer, prompt, generation_config)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# print(inp)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/generation/mt-bench.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBOT: \u001b[39m\u001b[39m\"\u001b[39m, output)\n",
      "File \u001b[0;32m/cephfs/home/kosenko/verbalist/verbalist/generation/generation_utils.py:52\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, prompt, generation_config)\u001b[0m\n\u001b[1;32m     45\u001b[0m data \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     46\u001b[0m     prompt,\n\u001b[1;32m     47\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m,\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m data \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m---> 52\u001b[0m output_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata, generation_config\u001b[39m=\u001b[39;49mgeneration_config)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     53\u001b[0m output_ids \u001b[39m=\u001b[39m output_ids[\u001b[39mlen\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]) :]\n\u001b[1;32m     54\u001b[0m output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/peft_model.py:975\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgeneration_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration_config\n\u001b[1;32m    974\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 975\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    976\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1606\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1590\u001b[0m         input_ids,\n\u001b[1;32m   1591\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1603\u001b[0m     )\n\u001b[1;32m   1604\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1605\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1607\u001b[0m         input_ids,\n\u001b[1;32m   1608\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1609\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1610\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1611\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1612\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1613\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1614\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1615\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1616\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1617\u001b[0m     )\n\u001b[1;32m   1619\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1620\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:2454\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2453\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2455\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2456\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2457\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2458\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2459\u001b[0m )\n\u001b[1;32m   2461\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2462\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1038\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1035\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1037\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1039\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1040\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1041\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1042\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1043\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1044\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1045\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1046\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1047\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1048\u001b[0m )\n\u001b[1;32m   1050\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    921\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    922\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    926\u001b[0m         hidden_states,\n\u001b[1;32m    927\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    928\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    929\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    930\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    931\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    932\u001b[0m         padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    935\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:649\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    647\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    648\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 649\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    650\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    652\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:247\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(down_proj)\n\u001b[1;32m    246\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgate_proj(x)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj(x))\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:441\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m x\u001b[39m.\u001b[39mdtype:\n\u001b[1;32m    439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 441\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mmatmul(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, bias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mhas_fp16_weights:\n\u001b[1;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mCB \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mCxB \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         \u001b[39m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[1;32m    446\u001b[0m         \u001b[39m# we no longer need the row-major weight\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:563\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m threshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    562\u001b[0m     state\u001b[39m.\u001b[39mthreshold \u001b[39m=\u001b[39m threshold\n\u001b[0;32m--> 563\u001b[0m \u001b[39mreturn\u001b[39;00m MatMul8bitLt\u001b[39m.\u001b[39;49mapply(A, B, out, bias, state)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:327\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(A\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    326\u001b[0m     A \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 327\u001b[0m CA, CAt, SCA, SCAt, coo_tensorA \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mdouble_quant(A\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat16), threshold\u001b[39m=\u001b[39;49mstate\u001b[39m.\u001b[39;49mthreshold)\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m state\u001b[39m.\u001b[39mthreshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m coo_tensorA \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mif\u001b[39;00m state\u001b[39m.\u001b[39mhas_fp16_weights:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/bitsandbytes/functional.py:1998\u001b[0m, in \u001b[0;36mdouble_quant\u001b[0;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001b[0m\n\u001b[1;32m   1995\u001b[0m     rows \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1997\u001b[0m \u001b[39mif\u001b[39;00m row_stats \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m col_stats \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1998\u001b[0m     row_stats, col_stats, nnz_row_ptr \u001b[39m=\u001b[39m get_colrow_absmax(\n\u001b[1;32m   1999\u001b[0m         A, threshold\u001b[39m=\u001b[39;49mthreshold\n\u001b[1;32m   2000\u001b[0m     )\n\u001b[1;32m   2002\u001b[0m \u001b[39mif\u001b[39;00m out_col \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2003\u001b[0m     out_col \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(A\u001b[39m.\u001b[39mshape, device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint8)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/bitsandbytes/functional.py:1881\u001b[0m, in \u001b[0;36mget_colrow_absmax\u001b[0;34m(A, row_stats, col_stats, nnz_block_ptr, threshold)\u001b[0m\n\u001b[1;32m   1879\u001b[0m ptrRowStats \u001b[39m=\u001b[39m get_ptr(row_stats)\n\u001b[1;32m   1880\u001b[0m ptrColStats \u001b[39m=\u001b[39m get_ptr(col_stats)\n\u001b[0;32m-> 1881\u001b[0m ptrNnzrows \u001b[39m=\u001b[39m get_ptr(nnz_block_ptr)\n\u001b[1;32m   1882\u001b[0m rows \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39mc_int32(rows)\n\u001b[1;32m   1883\u001b[0m cols \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39mc_int32(cols)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/bitsandbytes/functional.py:411\u001b[0m, in \u001b[0;36mget_ptr\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     \u001b[39mreturn\u001b[39;00m ct\u001b[39m.\u001b[39mc_void_p(A\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mdata_ptr())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    # conversation = VerbalistConversation(bot_token_id=12435)\n",
    "    conversation = VerbalistConversation()\n",
    "    for turn in item[\"turns_ru\"]:\n",
    "        conversation.add_user_message(turn)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "        # print(\"PROMPT\", prompt)\n",
    "        print(\"USER: \", turn)\n",
    "        generation_config = GenerationConfig(\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            pad_token_id=0,\n",
    "            max_new_tokens=512,\n",
    "            no_repeat_ngram_size=15,\n",
    "            repetition_penalty=1.1,\n",
    "            temperature=0.5,\n",
    "            top_k=30,\n",
    "            top_p=0.9,\n",
    "            # do_sample=True,\n",
    "            num_beams=1,\n",
    "        )\n",
    "        output = generate(model, tokenizer, prompt, generation_config)\n",
    "        # print(inp)\n",
    "        print(\"BOT: \", output)\n",
    "        conversation.add_bot_message(output)\n",
    "        print()\n",
    "        print(\"==============================\")\n",
    "        print()\n",
    "    # print()\n",
    "    # print(\"==============================\")\n",
    "    # print(\"==============================\")\n",
    "    print(\"==============================\")\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 5.88MB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 6.18MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.38MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 3.72MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 9225]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"PY007/TinyLlama-1.1B-intermediate-step-480k-1T\"\n",
    ")\n",
    "tokenizer.encode(\"bot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 616, 32002]), torch.Size([2, 616]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn([2, 616, 32002], requires_grad=True)\n",
    "target = torch.empty([2, 616], dtype=torch.long).random_(5)\n",
    "# output = loss(input, target)\n",
    "# output.shape\n",
    "input.shape, target.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "\t[1, 2, 3, 4],\n",
    "\t[1, 2, 3, 4],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.3530, 10.8364, 11.9819,  ..., 10.5342, 11.2929, 11.6620],\n",
       "        [11.7852,  8.2842, 11.6898,  ...,  8.9097, 11.5496, 13.1657]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(input.permute(0,2,1), target, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1230, 32002]), torch.Size([1230]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits = input[..., :-1, :].contiguous()\n",
    "shift_labels = target[..., 1:].contiguous()\n",
    "# Flatten the tokens\n",
    "shift_logits = shift_logits.view(-1, 32_002)\n",
    "shift_labels = shift_labels.view(-1)\n",
    "shift_logits.shape, shift_labels.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 615, 32002])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[..., :-1, :].contiguous().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1232"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "616*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1230])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.empty([2, 616], dtype=torch.long).random_(5)\n",
    "weight = weight[..., 1:].contiguous()\n",
    "weight = weight.view(-1)\n",
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37.8986,  0.0000,  0.0000,  ..., 16.7788, 22.9371, 10.2168],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(shift_logits, shift_labels, reduction=\"none\") * weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
