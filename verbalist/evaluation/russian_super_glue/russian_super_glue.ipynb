{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subset = \"lidirus\"\n",
    "dataset = load_dataset(\"RussianNLP/russian_super_glue\", subset)\n",
    "\n",
    "# dataset = dataset[\"validation\"]\n",
    "dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Кошка не сидела на коврике.',\n",
       " 'sentence2': 'Кошка сидела на коврике.',\n",
       " 'knowledge': '',\n",
       " 'lexical-semantics': '',\n",
       " 'logic': 'Negation',\n",
       " 'predicate-argument-structure': '',\n",
       " 'idx': 1,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Новая игровая консоль доступна по цене.',\n",
       " 'sentence2': 'Новая игровая консоль недоступна по цене.',\n",
       " 'knowledge': '',\n",
       " 'lexical-semantics': 'Morphological negation',\n",
       " 'logic': 'Negation',\n",
       " 'predicate-argument-structure': '',\n",
       " 'idx': 10,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Новая игровая консоль недоступна по цене.',\n",
       " 'sentence2': 'Новая игровая консоль доступна по цене.',\n",
       " 'knowledge': '',\n",
       " 'lexical-semantics': 'Morphological negation',\n",
       " 'logic': 'Negation',\n",
       " 'predicate-argument-structure': '',\n",
       " 'idx': 11,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from verbalist.generation.generation_utils import VerbalistConversation, generate\n",
    "\n",
    "weights_path = \"verbalist/model/models/verbalist_7b_v7/checkpoint-25900/adapter_model\"\n",
    "tokenizer_path = \"verbalist/model/models/verbalist_7b_v7/\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT <s> system\n",
      "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
      "<s> user\n",
      "Текст: \"\"\n",
      "Используя текст, можно ли сказать, что утверждение \"\" точно корректно относительно ситуации из текста? Ответь только \"да\" или \"нет\".\n",
      " </s> \n",
      "<s> bot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Да\n"
     ]
    }
   ],
   "source": [
    "# inputs = [\"Почему трава зеленая?\"]\n",
    "\n",
    "inputs = [\n",
    "    f\"\"\"Текст: \"\"\n",
    "Используя текст, можно ли сказать, что утверждение \"\" точно корректно относительно ситуации из текста? Ответь только \"да\" или \"нет\".\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "conversation = VerbalistConversation()\n",
    "conversation.add_user_message(inputs[0])\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "print(\"PROMPT\", prompt)\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    max_new_tokens=512,\n",
    "    # no_repeat_ngram_size=15,\n",
    "    repetition_penalty=1.1,\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    # do_sample=True,\n",
    ")\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "# print(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Кошка сидела на коврике.',\n",
       " 'sentence2': 'Кошка не сидела на коврике.',\n",
       " 'knowledge': '',\n",
       " 'lexical-semantics': '',\n",
       " 'logic': 'Negation',\n",
       " 'predicate-argument-structure': '',\n",
       " 'idx': 0,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lidirus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:11<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "rucos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "muserc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "terra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:14<00:00,  4.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "danetqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class EvalRussianSuperGlue:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name=\"danetqa\",\n",
    "        model_type=None,\n",
    "        model=None,\n",
    "        base_folder=None,\n",
    "        eval_name=None,\n",
    "        debug_mode=False,\n",
    "    ) -> None:\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = load_dataset(\"RussianNLP/russian_super_glue\", dataset_name)\n",
    "\n",
    "        if dataset_name in [\"lidirus\"]:\n",
    "            self.dataset = self.dataset[\"test\"]\n",
    "        else:\n",
    "            self.dataset = self.dataset[\"validation\"]\n",
    "\n",
    "        self.model = model\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.base_folder = Path(base_folder)\n",
    "        self.eval_name = eval_name\n",
    "\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "        if self.debug_mode:\n",
    "            num = 3\n",
    "            self.dataset = self.dataset.select(range(20, 20 + num))\n",
    "\n",
    "    def evaluate(self):\n",
    "        task_name = self.dataset_name\n",
    "        print(task_name)\n",
    "\n",
    "        eval_folder = self.base_folder / f\"{self.eval_name}\"\n",
    "        eval_folder.mkdir(exist_ok=True)\n",
    "        output_file = eval_folder / f\"{task_name}.jsonl\"\n",
    "        if output_file.is_file() and not self.debug_mode:\n",
    "            print(\"score file\")\n",
    "            predicts = []\n",
    "            ground_true = []\n",
    "            with open(output_file, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for i, line in enumerate(lines):\n",
    "                    line = json.loads(line)\n",
    "                    line[\"label\"] = int(line[\"label\"])\n",
    "                    predicts.append(line[\"label\"])\n",
    "                    gold_true = self.get_gold_true(item=dataset[i])\n",
    "                    ground_true.append(dataset[i][\"label\"])\n",
    "                acc = accuracy_score(ground_true, predicts)\n",
    "                print(f\"Accuracy: {acc}\")\n",
    "        else:\n",
    "            predicts = []\n",
    "            ground_true = []\n",
    "\n",
    "            with open(eval_folder / f\"{task_name}.log\", \"w\") as f:\n",
    "                idxs = []\n",
    "\n",
    "                for item in tqdm(self.dataset):\n",
    "                    prompt = self.get_prompt(item=item)\n",
    "                    result = self.get_answer(prompt=prompt)\n",
    "\n",
    "                    print(prompt, file=f)\n",
    "                    print(f\"predict answer = {result}\", file=f)\n",
    "                    print(f\"real answer = {item}\", file=f)\n",
    "\n",
    "                    answer = self.evaluate_answer(item=item, result=result)\n",
    "                    gold_true = self.get_gold_true(item=item)\n",
    "\n",
    "                    predicts.append(answer)\n",
    "                    ground_true.append(gold_true)\n",
    "                    idxs.append(item[\"idx\"])\n",
    "\n",
    "                acc = str(accuracy_score(ground_true, predicts))\n",
    "                print(f\"Accuracy: {acc}\")\n",
    "\n",
    "                with open(output_file, \"w\") as f:\n",
    "                    for idx, predict in zip(idxs, predicts):\n",
    "                        answer = {\n",
    "                            \"idx\": idx,\n",
    "                            \"label\": predict,\n",
    "                        }\n",
    "                        json.dump(answer, f)\n",
    "                        f.write(\"\\n\")\n",
    "                with open(eval_folder / f\"{task_name}.txt\", \"w\") as f:\n",
    "                    f.write(acc)\n",
    "\n",
    "    def get_answer(self, prompt):\n",
    "        models_map = {\n",
    "            \"verbalist\": self.verbalist_generation_1,\n",
    "        }\n",
    "        answer = models_map[self.model_type](prompt)\n",
    "        answer = answer.strip()\n",
    "        answer = answer.lower()\n",
    "        return answer\n",
    "\n",
    "    def get_gold_true(self, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": self.get_lidirus_true,\n",
    "            \"rucos\": lambda item: 1,\n",
    "            \"muserc\": lambda item: item[\"label\"],\n",
    "            \"terra\": lambda item: item[\"label\"],\n",
    "            \"danetqa\": lambda item: item[\"label\"],\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def get_lidirus_true(self, item):\n",
    "        return item[\"label\"]\n",
    "\n",
    "    def get_prompt(self, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": self.lidirus_prompt,\n",
    "            \"rucos\": self.rucos_prompt,\n",
    "            \"muserc\": self.muserc_prompt,\n",
    "            \"terra\": self.terra_prompt,\n",
    "            \"danetqa\": self.danetqa_prompt,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def evaluate_answer(self, result, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": self.lidirus_eval,\n",
    "            \"rucos\": self.rucos_eval,\n",
    "            \"muserc\": self.muserc_eval,\n",
    "            \"terra\": self.terra_eval,\n",
    "            \"danetqa\": self.danetqa_eval,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item, result=result)\n",
    "\n",
    "    def lidirus_prompt(self, item):\n",
    "        sentence1 = item[\"sentence1\"]\n",
    "        sentence2 = item[\"sentence2\"]\n",
    "        prompt = f\"\"\"Текст: \"{sentence1}\"\\nИспользуя текст, можно ли сказать, что утверждение \"{sentence2}\" точно корректно относительно ситуации из текста? Ответь только \"да\" или \"нет\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def rucos_prompt(self, item):\n",
    "        word_list = \", \".join([elem.strip() for elem in item[\"entities\"]])\n",
    "        text = item[\"passage\"]\n",
    "        prompt = f\"\"\"\\nТекст: {text}\\nЗапрос: {item['query']}\\nСписок слов: {word_list}\\nСогласно тексту, замени @placeholder запросе на наиболее подходящее слово из списка.\\nВ качестве ответа верни только одно слово.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def muserc_prompt(self, item):\n",
    "        prompt = f\"\"\"\\nтекст: {item['paragraph']}\\nвопрос: {item['question']}\\nЯвляется ли \"{item['answer']}\" правильным ответом на этот вопрос? Думай шаг за шагом. Основываясь на тексте, ответь только \"правильно\" или \"неправильно\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def terra_prompt(self, item):\n",
    "        prompt = f\"\"\"\\nконтекст: {item['premise']}\\nвывод: {item['hypothesis']}\\nявляется ли вывод правильным исходя из контекста? Думай шаг за шагом. В ответе напиши только \"правильный\" или \"неправильный\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def danetqa_prompt(self, item):\n",
    "        prompt = f\"{item['question']}\\nКонтекст: {item['passage']}\\nИспользуя контекст, ответь на вопрос используя только да или нет.\"\n",
    "        return prompt\n",
    "\n",
    "    def lidirus_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        if \"да\" in result:\n",
    "            answer = 1\n",
    "        elif \"не\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def rucos_eval(self, item=None, result=None):\n",
    "        answer = int(item[\"answers\"][0].lower() in result)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def muserc_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        # result = result\n",
    "        if \"неправил\" in result or \"не правил\" in result:\n",
    "            answer = 0\n",
    "        elif \"правил\" in result:\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def terra_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        # result = result\n",
    "        if \"неправил\" in result:\n",
    "            answer = 1\n",
    "        elif \"правил\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "    def danetqa_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "\n",
    "        if \"да\" in result:\n",
    "            answer = 1\n",
    "        elif \"не\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "    def verbalist_generation_1(self, prompt):\n",
    "        conversation = VerbalistConversation()\n",
    "        conversation.add_user_message(prompt)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "        # print(\"PROMPT\", prompt)\n",
    "        generation_config = GenerationConfig(\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            pad_token_id=0,\n",
    "            max_new_tokens=512,\n",
    "            # no_repeat_ngram_size=15,\n",
    "            repetition_penalty=1.1,\n",
    "            temperature=0.5,\n",
    "            top_k=40,\n",
    "            top_p=0.95,\n",
    "            # do_sample=True,\n",
    "        )\n",
    "        output = generate(\n",
    "            self.model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            generation_config,\n",
    "        )\n",
    "        # print(\"RESULT\", output)\n",
    "        return output\n",
    "\n",
    "\n",
    "for name in [\n",
    "    \"lidirus\",\n",
    "    \"rucos\",\n",
    "    \"muserc\",\n",
    "    \"terra\",\n",
    "    \"danetqa\",\n",
    "]:\n",
    "    evaluation = EvalRussianSuperGlue(\n",
    "        dataset_name=name,\n",
    "        model_type=\"verbalist\",\n",
    "        model=model,\n",
    "        base_folder=\"verbalist/evaluation/russian_super_glue/valid_evaluations/\",\n",
    "        eval_name=\"verbalist_7b_v7_checkpoint-25900\",\n",
    "        debug_mode=True,\n",
    "    )\n",
    "\n",
    "    evaluation.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
