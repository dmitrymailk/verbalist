{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.81M/3.81M [00:00<00:00, 5.95MB/s]\n",
      "Generating train split: 100%|██████████| 19845/19845 [00:01<00:00, 12646.78 examples/s]\n",
      "Generating validation split: 100%|██████████| 8505/8505 [00:00<00:00, 12721.75 examples/s]\n",
      "Generating test split: 100%|██████████| 18892/18892 [00:01<00:00, 13051.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subset = \"russe\"\n",
    "dataset = load_dataset(\"RussianNLP/russian_super_glue\", subset)\n",
    "\n",
    "dataset = dataset[\"validation\"]\n",
    "# dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'звание',\n",
       " 'sentence1': 'Будьте во всем достойными звания советского партизана',\n",
       " 'sentence2': 'Встретит вас гоф-фурьер такой-то (может быть, иначе называлось его звание – не помню)',\n",
       " 'start1': 26,\n",
       " 'start2': 67,\n",
       " 'end1': 33,\n",
       " 'end2': 74,\n",
       " 'gold_sense1': 2,\n",
       " 'gold_sense2': 1,\n",
       " 'idx': 1,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'золото',\n",
       " 'sentence1': 'Он взял здоровую кисть и золотой краской написал: «Смерть контрреволюции». Краска прошла сквозь материю, и буквы отпечатались золотом на обоях',\n",
       " 'sentence2': 'Стоя по колено в бледно-голубой холодной воде, я целый день промывал золото',\n",
       " 'start1': 126,\n",
       " 'start2': 69,\n",
       " 'end1': 134,\n",
       " 'end2': 76,\n",
       " 'gold_sense1': 2,\n",
       " 'gold_sense2': 1,\n",
       " 'idx': 10,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'евреи',\n",
       " 'sentence1': 'Мать моя так хотела учиться, что пробилась на выучку к раввину, хотя евреи учат только мальчиков',\n",
       " 'sentence2': 'Хотя считается, что у евреев должны быть черные волосы и карие глаза, среди них встречаются голубоглазые блондины',\n",
       " 'start1': 69,\n",
       " 'start2': 22,\n",
       " 'end1': 75,\n",
       " 'end2': 29,\n",
       " 'gold_sense1': 1,\n",
       " 'gold_sense2': 1,\n",
       " 'idx': 16,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'добродетель',\n",
       " 'sentence1': 'Отношения с хорошим и плохим, с пороками и добродетелью у художника чрезвычайно непростые',\n",
       " 'sentence2': 'Добродетель гибнет',\n",
       " 'start1': 43,\n",
       " 'start2': 0,\n",
       " 'end1': 56,\n",
       " 'end2': 12,\n",
       " 'gold_sense1': 1,\n",
       " 'gold_sense2': 1,\n",
       " 'idx': 17,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from verbalist.generation.generation_utils import VerbalistConversation, generate\n",
    "\n",
    "weights_path = \"verbalist/model/models/verbalist_7b_v7/checkpoint-25900/adapter_model\"\n",
    "tokenizer_path = \"verbalist/model/models/verbalist_7b_v7/\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT <s> system\n",
      "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
      "<s> user\n",
      "Предложение 1: Мать моя так хотела учиться, что пробилась на выучку к раввину, хотя евреи учат только мальчиков\n",
      "Предложение 2: Хотя считается, что у евреев должны быть черные волосы и карие глаза, среди них встречаются голубоглазые блондины\n",
      "Думай шаг за шагом. Является ли слово евреи одинаковым по значению и смыслу в этих двух предложениях. В ответе напиши 'да' или 'нет'.\n",
      " </s> \n",
      "<s> bot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нет, это не одно и то же слово. В первом предложении употребляется слово «еврей», а во втором - «евреи».\n"
     ]
    }
   ],
   "source": [
    "# inputs = [\"Почему трава зеленая?\"]\n",
    "\n",
    "inputs = [\n",
    "    f\"\"\"Предложение 1: Мать моя так хотела учиться, что пробилась на выучку к раввину, хотя евреи учат только мальчиков\n",
    "Предложение 2: Хотя считается, что у евреев должны быть черные волосы и карие глаза, среди них встречаются голубоглазые блондины\n",
    "Думай шаг за шагом. Является ли слово евреи одинаковым по значению и смыслу в этих двух предложениях. В ответе напиши 'да' или 'нет'.\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "conversation = VerbalistConversation()\n",
    "conversation.add_user_message(inputs[0])\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "print(\"PROMPT\", prompt)\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    max_new_tokens=512,\n",
    "    # no_repeat_ngram_size=15,\n",
    "    repetition_penalty=1.1,\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    # do_sample=True,\n",
    ")\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "# print(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Кошка сидела на коврике.',\n",
       " 'sentence2': 'Кошка не сидела на коврике.',\n",
       " 'knowledge': '',\n",
       " 'lexical-semantics': '',\n",
       " 'logic': 'Negation',\n",
       " 'predicate-argument-structure': '',\n",
       " 'idx': 0,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lidirus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      " 10%|█         | 1/10 [00:03<00:28,  3.16s/it]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class RussianSuperGluePrompts:\n",
    "    def lidirus_prompt(self, item):\n",
    "        sentence1 = item[\"sentence1\"]\n",
    "        sentence2 = item[\"sentence2\"]\n",
    "        prompt = f\"\"\"Текст: \"{sentence1}\"\\nИспользуя текст, можно ли сказать, что утверждение \"{sentence2}\" точно корректно относительно ситуации из текста? Ответь только \"да\" или \"нет\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def rucos_prompt(self, item):\n",
    "        word_list = \", \".join([elem.strip() for elem in item[\"entities\"]])\n",
    "        text = item[\"passage\"]\n",
    "        prompt = f\"\"\"\\nТекст: {text}\\nЗапрос: {item['query']}\\nСписок слов: {word_list}\\nСогласно тексту, замени @placeholder запросе на наиболее подходящее слово из списка.\\nВ качестве ответа верни только одно слово.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def muserc_prompt(self, item):\n",
    "        prompt = f\"\"\"\\nтекст: {item['paragraph']}\\nвопрос: {item['question']}\\nЯвляется ли \"{item['answer']}\" правильным ответом на этот вопрос? Думай шаг за шагом. Основываясь на тексте, ответь только \"правильно\" или \"неправильно\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def terra_prompt(self, item):\n",
    "        prompt = f\"\"\"\\nконтекст: {item['premise']}\\nвывод: {item['hypothesis']}\\nявляется ли вывод правильным исходя из контекста? Думай шаг за шагом. В ответе напиши только \"правильный\" или \"неправильный\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def danetqa_prompt(self, item):\n",
    "        prompt = f\"{item['question']}\\nКонтекст: {item['passage']}\\nИспользуя контекст, ответь на вопрос используя только да или нет.\"\n",
    "        return prompt\n",
    "\n",
    "    def parus_prompt(self, item):\n",
    "        cause = \"следствием \" if item[\"cause\"] == \"effect\" else \"причиной\"\n",
    "        prompt = f\"Текст: {item['premise']}\\nвыбор 1: {item['choice1']}\\nвыбор 2: {item['choice2']}\\nДумай шаг за шагом. Выбери вариант который послужил {cause} для поля 'Текст'. В ответе напиши 'выбор 1' или 'выбор 2'.\"\n",
    "        return prompt\n",
    "\n",
    "    def russe_prompt(self, item):\n",
    "        prompt = f\"\"\"Предложение 1: {item['sentence1']}\\nПредложение 2: {item['sentence2']}\\nДумай шаг за шагом. Является ли слово \"{item['word']}\" одинаковым по значению и смыслу в этих двух предложениях. В ответе напиши 'да' или 'нет'.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class RussianSuperGlueEval:\n",
    "    def lidirus_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        if \"да\" in result:\n",
    "            answer = 1\n",
    "        elif \"не\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def rucos_eval(self, item=None, result=None):\n",
    "        answer = int(item[\"answers\"][0].lower() in result)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def muserc_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        # result = result\n",
    "        if \"неправил\" in result or \"не правил\" in result:\n",
    "            answer = 0\n",
    "        elif \"правил\" in result:\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def terra_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        # result = result\n",
    "        if \"неправил\" in result:\n",
    "            answer = 1\n",
    "        elif \"правил\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "    def danetqa_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "\n",
    "        if \"да\" in result:\n",
    "            answer = 1\n",
    "        elif \"не\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "    def russe_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "\n",
    "        if \"нет\" in result:\n",
    "            answer = 0\n",
    "        elif \"да\" in result:\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "    def parus_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "\n",
    "        if \"1\" in result:\n",
    "            answer = 0\n",
    "        elif \"2\" in result:\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "\n",
    "class EvalRussianSuperGlue(RussianSuperGluePrompts, RussianSuperGlueEval):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name=\"danetqa\",\n",
    "        model_type=None,\n",
    "        model=None,\n",
    "        base_folder=None,\n",
    "        eval_name=None,\n",
    "        debug_mode=False,\n",
    "    ) -> None:\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = load_dataset(\"RussianNLP/russian_super_glue\", dataset_name)\n",
    "\n",
    "        if dataset_name in [\"lidirus\"]:\n",
    "            self.dataset = self.dataset[\"test\"]\n",
    "        else:\n",
    "            self.dataset = self.dataset[\"validation\"]\n",
    "\n",
    "        self.model = model\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.base_folder = Path(base_folder)\n",
    "        self.eval_name = eval_name\n",
    "\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "        if self.debug_mode:\n",
    "            num = 10\n",
    "            self.dataset = self.dataset.select(range(20, 20 + num))\n",
    "\n",
    "    def evaluate(self):\n",
    "        task_name = self.dataset_name\n",
    "        print(task_name)\n",
    "\n",
    "        eval_folder = self.base_folder / f\"{self.eval_name}\"\n",
    "        eval_folder.mkdir(exist_ok=True)\n",
    "        output_file = eval_folder / f\"{task_name}.jsonl\"\n",
    "        if output_file.is_file() and not self.debug_mode:\n",
    "            print(\"score file\")\n",
    "            predicts = []\n",
    "            ground_true = []\n",
    "            with open(output_file, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for i, line in enumerate(lines):\n",
    "                    line = json.loads(line)\n",
    "                    line[\"label\"] = int(line[\"label\"])\n",
    "                    predicts.append(line[\"label\"])\n",
    "                    gold_true = self.get_gold_true(item=dataset[i])\n",
    "                    ground_true.append(dataset[i][\"label\"])\n",
    "                acc = accuracy_score(ground_true, predicts)\n",
    "                print(f\"Accuracy: {acc}\")\n",
    "        else:\n",
    "            predicts = []\n",
    "            ground_true = []\n",
    "\n",
    "            with open(eval_folder / f\"{task_name}.log\", \"w\") as f:\n",
    "                idxs = []\n",
    "\n",
    "                for item in tqdm(self.dataset):\n",
    "                    prompt = self.get_prompt(item=item)\n",
    "                    result = self.get_answer(prompt=prompt)\n",
    "\n",
    "                    print(prompt, file=f)\n",
    "                    print(f\"predict answer = {result}\", file=f)\n",
    "                    print(f\"real answer = {item}\", file=f)\n",
    "\n",
    "                    answer = self.evaluate_answer(item=item, result=result)\n",
    "                    gold_true = self.get_gold_true(item=item)\n",
    "\n",
    "                    predicts.append(answer)\n",
    "                    ground_true.append(gold_true)\n",
    "                    idxs.append(item[\"idx\"])\n",
    "\n",
    "                acc = str(accuracy_score(ground_true, predicts))\n",
    "                print(f\"Accuracy: {acc}\")\n",
    "\n",
    "                with open(output_file, \"w\") as f:\n",
    "                    for idx, predict in zip(idxs, predicts):\n",
    "                        answer = {\n",
    "                            \"idx\": idx,\n",
    "                            \"label\": predict,\n",
    "                        }\n",
    "                        json.dump(answer, f)\n",
    "                        f.write(\"\\n\")\n",
    "                with open(eval_folder / f\"{task_name}.txt\", \"w\") as f:\n",
    "                    f.write(acc)\n",
    "\n",
    "    def get_answer(self, prompt):\n",
    "        models_map = {\n",
    "            \"verbalist\": self.verbalist_generation_1,\n",
    "        }\n",
    "        answer = models_map[self.model_type](prompt)\n",
    "        answer = answer.strip()\n",
    "        answer = answer.lower()\n",
    "        return answer\n",
    "\n",
    "    def get_gold_true(self, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": lambda item: item[\"label\"],\n",
    "            \"rucos\": lambda item: 1,\n",
    "            \"muserc\": lambda item: item[\"label\"],\n",
    "            \"terra\": lambda item: item[\"label\"],\n",
    "            \"danetqa\": lambda item: item[\"label\"],\n",
    "            \"parus\": lambda item: item[\"label\"],\n",
    "            \"russe\": lambda item: item[\"label\"],\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def get_prompt(self, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": self.lidirus_prompt,\n",
    "            \"rucos\": self.rucos_prompt,\n",
    "            \"muserc\": self.muserc_prompt,\n",
    "            \"terra\": self.terra_prompt,\n",
    "            \"danetqa\": self.danetqa_prompt,\n",
    "            \"parus\": self.parus_prompt,\n",
    "            \"russe\": self.russe_prompt,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def evaluate_answer(self, result, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": self.lidirus_eval,\n",
    "            \"rucos\": self.rucos_eval,\n",
    "            \"muserc\": self.muserc_eval,\n",
    "            \"terra\": self.terra_eval,\n",
    "            \"danetqa\": self.danetqa_eval,\n",
    "            \"parus\": self.parus_eval,\n",
    "            \"russe\": self.russe_eval,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item, result=result)\n",
    "\n",
    "    def verbalist_generation_1(self, prompt):\n",
    "        conversation = VerbalistConversation()\n",
    "        conversation.add_user_message(prompt)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "        # print(\"PROMPT\", prompt)\n",
    "        generation_config = GenerationConfig(\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            pad_token_id=0,\n",
    "            max_new_tokens=512,\n",
    "            # no_repeat_ngram_size=15,\n",
    "            repetition_penalty=1.1,\n",
    "            temperature=0.5,\n",
    "            top_k=40,\n",
    "            top_p=0.95,\n",
    "            # do_sample=True,\n",
    "        )\n",
    "        output = generate(\n",
    "            self.model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            generation_config,\n",
    "        )\n",
    "        # print(\"RESULT\", output)\n",
    "        return output\n",
    "\n",
    "\n",
    "for name in [\"lidirus\", \"rucos\", \"muserc\", \"terra\", \"danetqa\", \"parus\", \"russe\"]:\n",
    "    evaluation = EvalRussianSuperGlue(\n",
    "        dataset_name=name,\n",
    "        model_type=\"verbalist\",\n",
    "        model=model,\n",
    "        base_folder=\"verbalist/evaluation/russian_super_glue/valid_evaluations/\",\n",
    "        eval_name=\"verbalist_7b_v7_checkpoint-25900\",\n",
    "        debug_mode=True,\n",
    "    )\n",
    "\n",
    "    evaluation.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
