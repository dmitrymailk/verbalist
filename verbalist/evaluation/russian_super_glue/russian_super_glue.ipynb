{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.81M/3.81M [00:00<00:00, 5.95MB/s]\n",
      "Generating train split: 100%|██████████| 19845/19845 [00:01<00:00, 12646.78 examples/s]\n",
      "Generating validation split: 100%|██████████| 8505/8505 [00:00<00:00, 12721.75 examples/s]\n",
      "Generating test split: 100%|██████████| 18892/18892 [00:01<00:00, 13051.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subset = \"russe\"\n",
    "dataset = load_dataset(\"RussianNLP/russian_super_glue\", subset)\n",
    "\n",
    "dataset = dataset[\"validation\"]\n",
    "# dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'звание',\n",
       " 'sentence1': 'Будьте во всем достойными звания советского партизана',\n",
       " 'sentence2': 'Встретит вас гоф-фурьер такой-то (может быть, иначе называлось его звание – не помню)',\n",
       " 'start1': 26,\n",
       " 'start2': 67,\n",
       " 'end1': 33,\n",
       " 'end2': 74,\n",
       " 'gold_sense1': 2,\n",
       " 'gold_sense2': 1,\n",
       " 'idx': 1,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'золото',\n",
       " 'sentence1': 'Он взял здоровую кисть и золотой краской написал: «Смерть контрреволюции». Краска прошла сквозь материю, и буквы отпечатались золотом на обоях',\n",
       " 'sentence2': 'Стоя по колено в бледно-голубой холодной воде, я целый день промывал золото',\n",
       " 'start1': 126,\n",
       " 'start2': 69,\n",
       " 'end1': 134,\n",
       " 'end2': 76,\n",
       " 'gold_sense1': 2,\n",
       " 'gold_sense2': 1,\n",
       " 'idx': 10,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'евреи',\n",
       " 'sentence1': 'Мать моя так хотела учиться, что пробилась на выучку к раввину, хотя евреи учат только мальчиков',\n",
       " 'sentence2': 'Хотя считается, что у евреев должны быть черные волосы и карие глаза, среди них встречаются голубоглазые блондины',\n",
       " 'start1': 69,\n",
       " 'start2': 22,\n",
       " 'end1': 75,\n",
       " 'end2': 29,\n",
       " 'gold_sense1': 1,\n",
       " 'gold_sense2': 1,\n",
       " 'idx': 16,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'добродетель',\n",
       " 'sentence1': 'Отношения с хорошим и плохим, с пороками и добродетелью у художника чрезвычайно непростые',\n",
       " 'sentence2': 'Добродетель гибнет',\n",
       " 'start1': 43,\n",
       " 'start2': 0,\n",
       " 'end1': 56,\n",
       " 'end2': 12,\n",
       " 'gold_sense1': 1,\n",
       " 'gold_sense2': 1,\n",
       " 'idx': 17,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from verbalist.generation.generation_utils import VerbalistConversation, generate\n",
    "\n",
    "weights_path = \"verbalist/model/models/verbalist_7b_v7/checkpoint-25900/adapter_model\"\n",
    "tokenizer_path = \"verbalist/model/models/verbalist_7b_v7/\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT <s> system\n",
      "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
      "<s> user\n",
      "Предложение 1: Мать моя так хотела учиться, что пробилась на выучку к раввину, хотя евреи учат только мальчиков\n",
      "Предложение 2: Хотя считается, что у евреев должны быть черные волосы и карие глаза, среди них встречаются голубоглазые блондины\n",
      "Думай шаг за шагом. Является ли слово евреи одинаковым по значению и смыслу в этих двух предложениях. В ответе напиши 'да' или 'нет'.\n",
      " </s> \n",
      "<s> bot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нет, это не одно и то же слово. В первом предложении употребляется слово «еврей», а во втором - «евреи».\n"
     ]
    }
   ],
   "source": [
    "# inputs = [\"Почему трава зеленая?\"]\n",
    "\n",
    "inputs = [\n",
    "    f\"\"\"Предложение 1: Мать моя так хотела учиться, что пробилась на выучку к раввину, хотя евреи учат только мальчиков\n",
    "Предложение 2: Хотя считается, что у евреев должны быть черные волосы и карие глаза, среди них встречаются голубоглазые блондины\n",
    "Думай шаг за шагом. Является ли слово евреи одинаковым по значению и смыслу в этих двух предложениях. В ответе напиши 'да' или 'нет'.\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "conversation = VerbalistConversation()\n",
    "conversation.add_user_message(inputs[0])\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "print(\"PROMPT\", prompt)\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    max_new_tokens=512,\n",
    "    # no_repeat_ngram_size=15,\n",
    "    repetition_penalty=1.1,\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    # do_sample=True,\n",
    ")\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "# print(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Кошка сидела на коврике.',\n",
       " 'sentence2': 'Кошка не сидела на коврике.',\n",
       " 'knowledge': '',\n",
       " 'lexical-semantics': '',\n",
       " 'logic': 'Negation',\n",
       " 'predicate-argument-structure': '',\n",
       " 'idx': 0,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lidirus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [01:05<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4\n",
      "rucos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3\n",
      "muserc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [00:49<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "terra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [00:49<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "danetqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4\n",
      "parus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cause'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=274'>275</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mlidirus\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrucos\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmuserc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mterra\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdanetqa\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mparus\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrusse\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=275'>276</a>\u001b[0m     evaluation \u001b[39m=\u001b[39m EvalRussianSuperGlue(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=276'>277</a>\u001b[0m         dataset_name\u001b[39m=\u001b[39mname,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=277'>278</a>\u001b[0m         model_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mverbalist\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=281'>282</a>\u001b[0m         debug_mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=282'>283</a>\u001b[0m     )\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=284'>285</a>\u001b[0m     evaluation\u001b[39m.\u001b[39;49mevaluate()\n",
      "\u001b[1;32m/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=171'>172</a>\u001b[0m idxs \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=173'>174</a>\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m tqdm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=174'>175</a>\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_prompt(item\u001b[39m=\u001b[39;49mitem)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=175'>176</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_answer(prompt\u001b[39m=\u001b[39mprompt)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=177'>178</a>\u001b[0m     \u001b[39mprint\u001b[39m(prompt, file\u001b[39m=\u001b[39mf)\n",
      "\u001b[1;32m/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=223'>224</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_prompt\u001b[39m(\u001b[39mself\u001b[39m, item):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=224'>225</a>\u001b[0m     handlers_map \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=225'>226</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlidirus\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlidirus_prompt,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=226'>227</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrucos\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrucos_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=231'>232</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrusse\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrusse_prompt,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=232'>233</a>\u001b[0m     }\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=233'>234</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handlers_map[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_name](item\u001b[39m=\u001b[39;49mitem)\n",
      "\u001b[1;32m/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparus_prompt\u001b[39m(\u001b[39mself\u001b[39m, item):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     cause \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mследствием \u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m item[\u001b[39m\"\u001b[39;49m\u001b[39mcause\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39meffect\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mпричиной\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mТекст: \u001b[39m\u001b[39m{\u001b[39;00mitem[\u001b[39m'\u001b[39m\u001b[39mpremise\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mвыбор 1: \u001b[39m\u001b[39m{\u001b[39;00mitem[\u001b[39m'\u001b[39m\u001b[39mchoice1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mвыбор 2: \u001b[39m\u001b[39m{\u001b[39;00mitem[\u001b[39m'\u001b[39m\u001b[39mchoice2\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mДумай шаг за шагом. Выбери вариант который послужил \u001b[39m\u001b[39m{\u001b[39;00mcause\u001b[39m}\u001b[39;00m\u001b[39m для поля \u001b[39m\u001b[39m'\u001b[39m\u001b[39mТекст\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. В ответе напиши \u001b[39m\u001b[39m'\u001b[39m\u001b[39mвыбор 1\u001b[39m\u001b[39m'\u001b[39m\u001b[39m или \u001b[39m\u001b[39m'\u001b[39m\u001b[39mвыбор 2\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/kosenko/verbalist/verbalist/evaluation/russian_super_glue/russian_super_glue.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m prompt\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cause'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class RussianSuperGluePrompts:\n",
    "    def lidirus_prompt(self, item):\n",
    "        sentence1 = item[\"sentence1\"]\n",
    "        sentence2 = item[\"sentence2\"]\n",
    "        prompt = f\"\"\"Текст: \"{sentence1}\"\\nИспользуя текст, можно ли сказать, что утверждение \"{sentence2}\" точно корректно относительно ситуации из текста? Ответь только \"да\" или \"нет\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def rucos_prompt(self, item):\n",
    "        word_list = \", \".join([elem.strip() for elem in item[\"entities\"]])\n",
    "        text = item[\"passage\"]\n",
    "        prompt = f\"\"\"\\nТекст: {text}\\nЗапрос: {item['query']}\\nСписок слов: {word_list}\\nСогласно тексту, замени @placeholder запросе на наиболее подходящее слово из списка.\\nВ качестве ответа верни только одно слово.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def muserc_prompt(self, item):\n",
    "        prompt = f\"\"\"\\nтекст: {item['paragraph']}\\nвопрос: {item['question']}\\nЯвляется ли \"{item['answer']}\" правильным ответом на этот вопрос? Думай шаг за шагом. Основываясь на тексте, ответь только \"правильно\" или \"неправильно\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def terra_prompt(self, item):\n",
    "        prompt = f\"\"\"\\nконтекст: {item['premise']}\\nвывод: {item['hypothesis']}\\nявляется ли вывод правильным исходя из контекста? Думай шаг за шагом. В ответе напиши только \"правильный\" или \"неправильный\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def danetqa_prompt(self, item):\n",
    "        prompt = f\"{item['question']}\\nКонтекст: {item['passage']}\\nИспользуя контекст, ответь на вопрос используя только да или нет.\"\n",
    "        return prompt\n",
    "\n",
    "    def parus_prompt(self, item):\n",
    "        cause = \"следствием \" if item[\"question\"] == \"effect\" else \"причиной\"\n",
    "        prompt = f\"Текст: {item['premise']}\\nвыбор 1: {item['choice1']}\\nвыбор 2: {item['choice2']}\\nДумай шаг за шагом. Выбери вариант который послужил {cause} для поля 'Текст'. В ответе напиши 'выбор 1' или 'выбор 2'.\"\n",
    "        return prompt\n",
    "\n",
    "    def russe_prompt(self, item):\n",
    "        prompt = f\"\"\"Предложение 1: {item['sentence1']}\\nПредложение 2: {item['sentence2']}\\nДумай шаг за шагом. Является ли слово \"{item['word']}\" одинаковым по значению и смыслу в этих двух предложениях. В ответе напиши 'да' или 'нет'.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class RussianSuperGlueEval:\n",
    "    def lidirus_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        if \"да\" in result:\n",
    "            answer = 1\n",
    "        elif \"не\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def rucos_eval(self, item=None, result=None):\n",
    "        answer = int(item[\"answers\"][0].lower() in result)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def muserc_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        # result = result\n",
    "        if \"неправил\" in result or \"не правил\" in result:\n",
    "            answer = 0\n",
    "        elif \"правил\" in result:\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def terra_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        # result = result\n",
    "        if \"неправил\" in result:\n",
    "            answer = 1\n",
    "        elif \"правил\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "    def danetqa_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "\n",
    "        if \"да\" in result:\n",
    "            answer = 1\n",
    "        elif \"не\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "    def russe_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "\n",
    "        if \"нет\" in result:\n",
    "            answer = 0\n",
    "        elif \"да\" in result:\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "    def parus_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "\n",
    "        if \"1\" in result:\n",
    "            answer = 0\n",
    "        elif \"2\" in result:\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "\n",
    "class EvalRussianSuperGlue(RussianSuperGluePrompts, RussianSuperGlueEval):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name=\"danetqa\",\n",
    "        model_type=None,\n",
    "        model=None,\n",
    "        base_folder=None,\n",
    "        eval_name=None,\n",
    "        debug_mode=False,\n",
    "    ) -> None:\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = load_dataset(\"RussianNLP/russian_super_glue\", dataset_name)\n",
    "\n",
    "        if dataset_name in [\"lidirus\"]:\n",
    "            self.dataset = self.dataset[\"test\"]\n",
    "        else:\n",
    "            self.dataset = self.dataset[\"validation\"]\n",
    "\n",
    "        self.model = model\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.base_folder = Path(base_folder)\n",
    "        self.eval_name = eval_name\n",
    "\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "        if self.debug_mode:\n",
    "            num = 10\n",
    "            self.dataset = self.dataset.select(range(20, 20 + num))\n",
    "\n",
    "    def evaluate(self):\n",
    "        task_name = self.dataset_name\n",
    "        print(task_name)\n",
    "\n",
    "        eval_folder = self.base_folder / f\"{self.eval_name}\"\n",
    "        eval_folder.mkdir(exist_ok=True)\n",
    "        output_file = eval_folder / f\"{task_name}.jsonl\"\n",
    "        if output_file.is_file() and not self.debug_mode:\n",
    "            print(\"score file\")\n",
    "            predicts = []\n",
    "            ground_true = []\n",
    "            with open(output_file, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for i, line in enumerate(lines):\n",
    "                    line = json.loads(line)\n",
    "                    line[\"label\"] = int(line[\"label\"])\n",
    "                    predicts.append(line[\"label\"])\n",
    "                    gold_true = self.get_gold_true(item=dataset[i])\n",
    "                    ground_true.append(dataset[i][\"label\"])\n",
    "                acc = accuracy_score(ground_true, predicts)\n",
    "                print(f\"Accuracy: {acc}\")\n",
    "        else:\n",
    "            predicts = []\n",
    "            ground_true = []\n",
    "\n",
    "            with open(eval_folder / f\"{task_name}.log\", \"w\") as f:\n",
    "                idxs = []\n",
    "\n",
    "                for item in tqdm(self.dataset):\n",
    "                    prompt = self.get_prompt(item=item)\n",
    "                    result = self.get_answer(prompt=prompt)\n",
    "\n",
    "                    print(prompt, file=f)\n",
    "                    print(f\"predict answer = {result}\", file=f)\n",
    "                    print(f\"real answer = {item}\", file=f)\n",
    "\n",
    "                    answer = self.evaluate_answer(item=item, result=result)\n",
    "                    gold_true = self.get_gold_true(item=item)\n",
    "\n",
    "                    predicts.append(answer)\n",
    "                    ground_true.append(gold_true)\n",
    "                    idxs.append(item[\"idx\"])\n",
    "\n",
    "                acc = str(accuracy_score(ground_true, predicts))\n",
    "                print(f\"Accuracy: {acc}\")\n",
    "\n",
    "                with open(output_file, \"w\") as f:\n",
    "                    for idx, predict in zip(idxs, predicts):\n",
    "                        answer = {\n",
    "                            \"idx\": idx,\n",
    "                            \"label\": predict,\n",
    "                        }\n",
    "                        json.dump(answer, f)\n",
    "                        f.write(\"\\n\")\n",
    "                with open(eval_folder / f\"{task_name}.txt\", \"w\") as f:\n",
    "                    f.write(acc)\n",
    "\n",
    "    def get_answer(self, prompt):\n",
    "        models_map = {\n",
    "            \"verbalist\": self.verbalist_generation_1,\n",
    "        }\n",
    "        answer = models_map[self.model_type](prompt)\n",
    "        answer = answer.strip()\n",
    "        answer = answer.lower()\n",
    "        return answer\n",
    "\n",
    "    def get_gold_true(self, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": lambda item: item[\"label\"],\n",
    "            \"rucos\": lambda item: 1,\n",
    "            \"muserc\": lambda item: item[\"label\"],\n",
    "            \"terra\": lambda item: item[\"label\"],\n",
    "            \"danetqa\": lambda item: item[\"label\"],\n",
    "            \"parus\": lambda item: item[\"label\"],\n",
    "            \"russe\": lambda item: item[\"label\"],\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def get_prompt(self, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": self.lidirus_prompt,\n",
    "            \"rucos\": self.rucos_prompt,\n",
    "            \"muserc\": self.muserc_prompt,\n",
    "            \"terra\": self.terra_prompt,\n",
    "            \"danetqa\": self.danetqa_prompt,\n",
    "            \"parus\": self.parus_prompt,\n",
    "            \"russe\": self.russe_prompt,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def evaluate_answer(self, result, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": self.lidirus_eval,\n",
    "            \"rucos\": self.rucos_eval,\n",
    "            \"muserc\": self.muserc_eval,\n",
    "            \"terra\": self.terra_eval,\n",
    "            \"danetqa\": self.danetqa_eval,\n",
    "            \"parus\": self.parus_eval,\n",
    "            \"russe\": self.russe_eval,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item, result=result)\n",
    "\n",
    "    def verbalist_generation_1(self, prompt):\n",
    "        conversation = VerbalistConversation()\n",
    "        conversation.add_user_message(prompt)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "        # print(\"PROMPT\", prompt)\n",
    "        generation_config = GenerationConfig(\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            pad_token_id=0,\n",
    "            max_new_tokens=512,\n",
    "            # no_repeat_ngram_size=15,\n",
    "            repetition_penalty=1.1,\n",
    "            temperature=0.5,\n",
    "            top_k=40,\n",
    "            top_p=0.95,\n",
    "            # do_sample=True,\n",
    "        )\n",
    "        output = generate(\n",
    "            self.model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            generation_config,\n",
    "        )\n",
    "        # print(\"RESULT\", output)\n",
    "        return output\n",
    "\n",
    "\n",
    "for name in [\"lidirus\", \"rucos\", \"muserc\", \"terra\", \"danetqa\", \"parus\", \"russe\"]:\n",
    "    evaluation = EvalRussianSuperGlue(\n",
    "        dataset_name=name,\n",
    "        model_type=\"verbalist\",\n",
    "        model=model,\n",
    "        base_folder=\"verbalist/evaluation/russian_super_glue/valid_evaluations/\",\n",
    "        eval_name=\"verbalist_7b_v7_checkpoint-25900\",\n",
    "        debug_mode=True,\n",
    "    )\n",
    "\n",
    "    evaluation.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
