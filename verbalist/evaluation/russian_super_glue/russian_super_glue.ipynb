{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://russiansuperglue.com/ru/tasks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# subset = \"russe\"\n",
    "# subset = \"lidirus\"\n",
    "# subset = \"rcb\"\n",
    "# subset = \"muserc\"\n",
    "# subset = \"terra\"\n",
    "subset = \"rwsd\"\n",
    "# subset = \"danetqa\"\n",
    "# subset = \"rucos\"\n",
    "dataset = load_dataset(\"RussianNLP/russian_super_glue\", subset)\n",
    "\n",
    "dataset = dataset[\"validation\"]\n",
    "# dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verbalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 17.01s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT <s> system\n",
      "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
      "<s> user\n",
      "Почему трава зеленая? </s> \n",
      "<s> bot\n",
      "Зеленый цвет у многих растений обусловлен наличием в них хлорофилла, пигмента, который позволяет растениям производить энергию посредством фотосинтеза. Хлорофилл отражает красные и синие световые длины волн, оставляя зеленый цвет.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from verbalist.generation.generation_utils import VerbalistConversation, generate\n",
    "\n",
    "weights_path = \"verbalist/model/models/verbalist_7b_v9/checkpoint-800/adapter_model\"\n",
    "tokenizer_path = \"verbalist/model/models/verbalist_7b_v9/\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "inputs = [\"Почему трава зеленая?\"]\n",
    "\n",
    "# inputs = [\n",
    "#     f\"\"\"Предложение 1: Мать моя так хотела учиться, что пробилась на выучку к раввину, хотя евреи учат только мальчиков\n",
    "# Предложение 2: Хотя считается, что у евреев должны быть черные волосы и карие глаза, среди них встречаются голубоглазые блондины\n",
    "# Думай шаг за шагом. Является ли слово евреи одинаковым по значению и смыслу в этих двух предложениях. В ответе напиши 'да' или 'нет'.\n",
    "# \"\"\"\n",
    "# ]\n",
    "\n",
    "conversation = VerbalistConversation(\n",
    "    bot_token_id=12435,\n",
    ")\n",
    "conversation.add_user_message(inputs[0])\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "print(\"PROMPT\", prompt)\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    max_new_tokens=512,\n",
    "    # no_repeat_ngram_size=15,\n",
    "    repetition_penalty=1.1,\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    # do_sample=True,\n",
    ")\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "# print(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT <s> system\n",
      "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
      "<s> user\n",
      "Почему трава зеленая? </s> \n",
      "<s> bot\n",
      "Зеленый цвет у многих растений обусловлен наличием в них хлорофилла, пигмента, который позволяет растениям производить энергию посредством фотосинтеза. Хлорофилл отражает красные и синие световые длины волн, оставляя зеленый цвет.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.79s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 1536,\n",
      "  \"no_repeat_ngram_size\": 15,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.2,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Почему трава зеленая?\n",
      "Зеленый цвет у растений обусловлен наличием в них хлорофилла - пигмента, который позволяет им проводить фотосинтез. Фотосинтез - это процесс, благодаря которому растения превращают солнечную энергию в химическую энергию, используя углекислый газ из воздуха и воду. Хлорофилл отражает свет, который не используется для фотосинтеза, и поглощает свет, который используется для фотосинтеза. Этот процесс приводит к образованию кислорода и глюкозы, которые являются основными продуктами фотосинтеза. Зеленый цвет растений является результатом комбинации этих двух цветов.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE,\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\"role\": \"bot\", \"content\": message})\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    with torch.no_grad():\n",
    "        data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        data = data.to(model.device)\n",
    "        output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "        output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "        output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        return output.strip()\n",
    "\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, MODEL_NAME, torch_dtype=torch.float16)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "\n",
    "inputs = [\n",
    "    \"Почему трава зеленая?\",\n",
    "    # \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\",\n",
    "]\n",
    "for inp in inputs:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    print(inp)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "openai.api_key = open(\"./chat_gpt_token\").read()\n",
    "\n",
    "\n",
    "def chat_with_chatgpt(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    chat_completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    return chat_completion[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "user_prompt = \"Hello world.\"\n",
    "chatbot_response = chat_with_chatgpt(user_prompt)\n",
    "print(chatbot_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_chatgpt(\n",
    "    \"\"\"\n",
    "Текст: Однажды Дик дразнил жеребят и не знал, что хозяин был на соседнем поле; но он был там, наблюдая за происходящим; он перепрыгнул через изгородь одним махом и, схватив Дика за руку, ударил его так сильно, что он взревел от боли и удивления.\n",
    "Отвечай согласно тексту. Связаны ли \"хозяин\" с \"он перепрыгнул\"? В ответе напиши только \"да\" или \"нет\"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any([True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiDiRus\n",
      "0.3903985507246377\n",
      "RCB\n",
      "0.0\n",
      "PARus\n",
      "0.0\n",
      "MuSeRC_flat\n",
      "0.0\n",
      "TERRa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3198/3198 [13:33<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.11819887429643527\n",
      "RUSSE\n",
      "0.6080351471522337\n",
      "RWSD\n",
      "0.0\n",
      "DaNetQA\n",
      "0.0\n",
      "RuCoS\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class RussianSuperGluePrompts:\n",
    "    def lidirus_prompt(self, item):\n",
    "        sentence1 = item[\"sentence1\"]\n",
    "        sentence2 = item[\"sentence2\"]\n",
    "        prompt = f\"\"\"Текст: \"{sentence1}\"\\nИспользуя текст, можно ли сказать, что утверждение \"{sentence2}\" корректно относительно ситуации из текста? Ответь только \"да\" или \"нет\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def rcb_prompt(self, item):\n",
    "        sentence1 = item[\"premise\"]\n",
    "        sentence2 = item[\"hypothesis\"]\n",
    "        prompt = f\"\"\"premise: {sentence1}\\nhypothesis: {sentence2}\\nКак связаны между собой premise и hypothesis? Это contradiction, entailment или neutral? Ответь одним словом.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def parus_prompt(self, item):\n",
    "        cause = \"следствием \" if item[\"question\"] == \"effect\" else \"причиной\"\n",
    "        prompt = f\"Текст: {item['premise']}\\nвыбор 1: {item['choice1']}\\nвыбор 2: {item['choice2']}\\nВыбери вариант который послужил {cause} для поля 'Текст'. В ответе напиши 'выбор 1' или 'выбор 2'.\"\n",
    "        return prompt\n",
    "\n",
    "    def muserc_prompt(self, item):\n",
    "        prompt = f\"\"\"Текст: {item['paragraph']}\\nвопрос: {item['question']}\\nЯвляется ли \"{item['answer']}\" правильным ответом на этот вопрос? Основываясь на только тексте, ответь \"правильно\" или \"неправильно\" \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def rucos_prompt(self, item):\n",
    "        word_list = \", \".join([elem.strip() for elem in item[\"entities\"]])\n",
    "        text = item[\"passage\"]\n",
    "        prompt = f\"\"\"\\nТекст: {text}\\nЗапрос: {item['query']}\\nСписок слов: {word_list}\\nСогласно тексту, замени @placeholder запросе на наиболее подходящее слово из списка.\\nВ качестве ответа верни только одно слово.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def terra_prompt(self, item):\n",
    "        prompt = f\"\"\"Контекст: {item['premise']}\\nВывод: {item['hypothesis']}\\nЯвляется ли вывод правильным исходя из контекста? Думай шаг за шагом. В ответе напиши только \"правильный\" или \"неправильный\" \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def russe_prompt(self, item):\n",
    "        prompt = f\"\"\"Предложение 1: {item['sentence1']}\\nПредложение 2: {item['sentence2']}\\nЯвляется ли слово \"{item['word']}\" одинаковым по значению и смыслу в этих двух предложениях. В ответе напиши 'да' или 'нет'.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def rwsd_prompt(self, item):\n",
    "        prompt = f\"\"\"Текст: {item['text']}\\nОтвечай согласно тексту. Связаны ли \"{item['span1_text']}\" с \"{item['span2_text']}\"? В ответе напиши только \"да\" или \"нет\" \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def danetqa_prompt(self, item):\n",
    "        prompt = f\"{item['question']}\\nКонтекст: {item['passage']}\\nИспользуя контекст, ответь на вопрос используя только да или нет.\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class RussianSuperGlueEval:\n",
    "    def lidirus_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "\n",
    "        result = result.lower()\n",
    "        if \"да\" in result:\n",
    "            answer = 1\n",
    "        elif \"не\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        answer = \"entailment\" if answer == 1 else \"not_entailment\"\n",
    "        return answer\n",
    "\n",
    "    def rcb_eval(self, item=None, result=None):\n",
    "        answer = item[\"label\"]\n",
    "        answer_map = {\n",
    "            0: \"entailment\",\n",
    "            1: \"contradiction\",\n",
    "            2: \"neutral\",\n",
    "            -1: \"neutral\",\n",
    "        }\n",
    "        answer = answer_map[answer]\n",
    "\n",
    "        result = result.lower().replace(\".\", \"\")\n",
    "\n",
    "        if result in [\"neutral\", \"entailment\", \"contradiction\"]:\n",
    "            return result\n",
    "        else:\n",
    "            incorrect_answer = \"neutral\"\n",
    "            return incorrect_answer\n",
    "\n",
    "    def parus_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "\n",
    "        if \"1\" in result:\n",
    "            answer = 0\n",
    "        elif \"2\" in result:\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "    def muserc_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        result = result.lower()\n",
    "        words_0 = [\n",
    "            \"неправил\",\n",
    "            \"не правил\",\n",
    "            \"нет,\",\n",
    "        ]\n",
    "        words_1 = [\n",
    "            \"правил\",\n",
    "            \"да,\",\n",
    "        ]\n",
    "\n",
    "        if any([item in result for item in words_0]):\n",
    "            answer = 0\n",
    "        elif any([item in result for item in words_1]):\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def terra_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        result = result.lower()\n",
    "\n",
    "        words_0 = [\n",
    "            \"неправил\",\n",
    "            \"неверно\",\n",
    "            \"нет,\",\n",
    "        ]\n",
    "        words_1 = [\"правил\", \"да,\", \"правд\"]\n",
    "\n",
    "        if any([item in result for item in words_0]):\n",
    "            answer = \"not_entailment\"\n",
    "        elif any([item in result for item in words_1]):\n",
    "            answer = \"entailment\"\n",
    "        else:\n",
    "            answer = \"not_entailment\"\n",
    "        return answer\n",
    "\n",
    "    def russe_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        result = result.lower()\n",
    "\n",
    "        words_0 = [\n",
    "            \"неправил\",\n",
    "            \"неверно\",\n",
    "            \"нет,\",\n",
    "            \"нет.\",\n",
    "        ]\n",
    "        words_1 = [\n",
    "            \"правил\",\n",
    "            \"да,\",\n",
    "            \"да.\",\n",
    "        ]\n",
    "\n",
    "        if any([item in result for item in words_0]):\n",
    "            answer = \"false\"\n",
    "        elif any([item in result for item in words_1]):\n",
    "            answer = \"true\"\n",
    "        else:\n",
    "            answer = \"false\"\n",
    "        return answer\n",
    "\n",
    "    def rwsd_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        result = result.lower()\n",
    "\n",
    "        words_0 = [\n",
    "            \"неправил\",\n",
    "            \"неверно\",\n",
    "            \"нет,\",\n",
    "            \"нет.\",\n",
    "        ]\n",
    "        words_1 = [\n",
    "            \"правил\",\n",
    "            \"да,\",\n",
    "            \"да.\",\n",
    "        ]\n",
    "\n",
    "        if any([item in result for item in words_0]):\n",
    "            answer = \"False\"\n",
    "        elif any([item in result for item in words_1]):\n",
    "            answer = \"True\"\n",
    "        else:\n",
    "            answer = \"False\"\n",
    "        return answer\n",
    "\n",
    "    def rucos_eval(self, item=None, result=None):\n",
    "        if self.split == \"test\":\n",
    "            return result\n",
    "        result = result.lower().replace(\".\", \"\")\n",
    "        for true_answer in item[\"answers\"]:\n",
    "            true_answer = true_answer.lower().replace(\".\", \"\")\n",
    "            # print(true_answer, result)\n",
    "            if true_answer in result:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def danetqa_eval(self, item=None, result=None):\n",
    "        answer = None\n",
    "        result = result.lower()\n",
    "\n",
    "        words_0 = [\n",
    "            \"неправил\",\n",
    "            \"неверно\",\n",
    "            \"нет,\",\n",
    "            \"нет.\",\n",
    "        ]\n",
    "        words_1 = [\n",
    "            \"правил\",\n",
    "            \"да,\",\n",
    "            \"да.\",\n",
    "        ]\n",
    "\n",
    "        if any([item in result for item in words_1]):\n",
    "            answer = \"true\"\n",
    "        elif any([item in result for item in words_0]):\n",
    "            answer = \"false\"\n",
    "        else:\n",
    "            answer = \"false\"\n",
    "        return answer\n",
    "\n",
    "\n",
    "class RussianSuperGlueModels:\n",
    "    @staticmethod\n",
    "    def verbalist_generation(\n",
    "        prompt=None,\n",
    "        model=None,\n",
    "        bot_token_id=9225,\n",
    "        generation_config=None,\n",
    "    ):\n",
    "        conversation = VerbalistConversation(bot_token_id=bot_token_id)\n",
    "        conversation.add_user_message(prompt)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "        # print(\"PROMPT\", prompt)\n",
    "\n",
    "        output = generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            generation_config,\n",
    "        )\n",
    "        # print(\"RESULT\", output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def saiga_mistral(prompt=None, model=None, generation_config=None):\n",
    "        conversation = Conversation()\n",
    "        conversation.add_user_message(prompt)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "        output = generate(model, tokenizer, prompt, generation_config)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def chat_gpt(prompt=None):\n",
    "        chat_completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "\n",
    "        return chat_completion[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "    def dummy(self, prompt=None):\n",
    "        tasks_map = {\n",
    "            \"lidirus\": \"да\",\n",
    "            \"rcb\": \"entailment\",\n",
    "            \"parus\": \"1\",\n",
    "            \"muserc\": \"правил\",\n",
    "            \"terra\": \"правил\",\n",
    "            \"russe\": \"правил\",\n",
    "            \"rwsd\": \"да\",\n",
    "            \"danetqa\": \"да\",\n",
    "            \"rucos\": \"Тест\",\n",
    "        }\n",
    "        return tasks_map[self.dataset_name]\n",
    "\n",
    "\n",
    "class EvalRussianSuperGlue(\n",
    "    RussianSuperGluePrompts,\n",
    "    RussianSuperGlueEval,\n",
    "    RussianSuperGlueModels,\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name=\"danetqa\",\n",
    "        model_type=None,\n",
    "        base_folder=None,\n",
    "        generation_function=None,\n",
    "        split=None,\n",
    "        eval_name=None,\n",
    "        debug_mode=False,\n",
    "    ) -> None:\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = load_dataset(\"RussianNLP/russian_super_glue\", dataset_name)\n",
    "        self.split = split\n",
    "\n",
    "        if dataset_name in [\"lidirus\"]:\n",
    "            self.dataset = self.dataset[\"test\"]\n",
    "        else:\n",
    "            if split != \"test\":\n",
    "                self.dataset = self.dataset[\"validation\"]\n",
    "            else:\n",
    "                self.dataset = self.dataset[\"test\"]\n",
    "\n",
    "        self.generation_function = generation_function\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.base_folder = Path(base_folder)\n",
    "        self.eval_name = eval_name\n",
    "\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "        if self.debug_mode:\n",
    "            num = 10\n",
    "            self.dataset = self.dataset.select(range(20, 20 + num))\n",
    "\n",
    "        self.eval_filenames = {\n",
    "            \"lidirus\": \"LiDiRus\",\n",
    "            \"rcb\": \"RCB\",\n",
    "            \"parus\": \"PARus\",\n",
    "            \"muserc\": \"MuSeRC_flat\",\n",
    "            \"terra\": \"TERRa\",\n",
    "            \"russe\": \"RUSSE\",\n",
    "            \"rwsd\": \"RWSD\",\n",
    "            \"danetqa\": \"DaNetQA\",\n",
    "            \"rucos\": \"RuCoS\",\n",
    "        }\n",
    "\n",
    "    def evaluate(self):\n",
    "        task_name = self.eval_filenames[self.dataset_name]\n",
    "        print(task_name)\n",
    "        split_name = self.split\n",
    "        if self.debug_mode:\n",
    "            split_name = \"debug\"\n",
    "\n",
    "        eval_folder = self.base_folder / f\"{self.eval_name}\" / split_name\n",
    "        eval_folder.mkdir(exist_ok=True, parents=True)\n",
    "        output_file = eval_folder / f\"{task_name}.jsonl\"\n",
    "\n",
    "        if output_file.is_file() and not self.debug_mode and self.model_type != \"dummy\":\n",
    "            with open(eval_folder / f\"{task_name}.txt\", \"r\") as f:\n",
    "                print(f.read())\n",
    "        else:\n",
    "            predicts = []\n",
    "            ground_true = []\n",
    "\n",
    "            with open(eval_folder / f\"{task_name}.log\", \"w\") as f:\n",
    "                idxs = []\n",
    "\n",
    "                for item in tqdm(self.dataset):\n",
    "                    prompt = self.get_prompt(item=item)\n",
    "                    result = self.get_answer(prompt=prompt)\n",
    "\n",
    "                    print(prompt, file=f)\n",
    "                    print(f\"predict answer = {result}\", file=f)\n",
    "                    if \"label\" in item:\n",
    "                        print(f\"real answer = {item['label']}\", file=f)\n",
    "                    else:\n",
    "                        print(f\"real answer = {item['answers']}\", file=f)\n",
    "\n",
    "                    answer = self.evaluate_answer(item=item, result=result)\n",
    "                    gold_true = self.get_gold_true(item=item)\n",
    "\n",
    "                    predicts.append(answer)\n",
    "                    ground_true.append(gold_true)\n",
    "                    idxs.append(item[\"idx\"])\n",
    "                acc = None\n",
    "                if self.dataset_name != \"rucos\":\n",
    "                    acc = str(accuracy_score(ground_true, predicts))\n",
    "                else:\n",
    "                    acc = str(0)\n",
    "                    if self.split == \"valid\":\n",
    "                        acc = str(accuracy_score(ground_true, predicts))\n",
    "                        # print(ground_true, predicts)\n",
    "                    idxs = [item[\"query\"] for item in idxs]\n",
    "\n",
    "                print(f\"Accuracy: {acc}\")\n",
    "\n",
    "                with open(output_file, \"w\") as f:\n",
    "                    for idx, predict in zip(idxs, predicts):\n",
    "                        answer = {\n",
    "                            \"idx\": idx,\n",
    "                            \"label\": predict,\n",
    "                        }\n",
    "                        json.dump(answer, f, ensure_ascii=False)\n",
    "                        f.write(\"\\n\")\n",
    "                if self.dataset_name == \"muserc\":\n",
    "                    self.save_muserc(\n",
    "                        path_flat=output_file,\n",
    "                        save_path=eval_folder / f\"MuSeRC.jsonl\",\n",
    "                    )\n",
    "                with open(eval_folder / f\"{task_name}.txt\", \"w\") as f:\n",
    "                    f.write(acc)\n",
    "\n",
    "    def get_answer(self, prompt):\n",
    "        answer = None\n",
    "        if self.model_type == \"dummy\":\n",
    "            answer = self.dummy(prompt=prompt)\n",
    "        else:\n",
    "            answer = self.generation_function(prompt=prompt)\n",
    "        answer = answer.strip()\n",
    "        return answer\n",
    "\n",
    "    def get_gold_true(self, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": lambda item: {\n",
    "                1: \"entailment\",\n",
    "                0: \"not_entailment\",\n",
    "            }[item[\"label\"]],\n",
    "            \"rcb\": lambda item: {\n",
    "                0: \"entailment\",\n",
    "                1: \"contradiction\",\n",
    "                2: \"neutral\",\n",
    "                -1: \"neutral\",\n",
    "            }[item[\"label\"]],\n",
    "            \"rucos\": self.rucos_gold,\n",
    "            \"muserc\": lambda item: item[\"label\"],\n",
    "            \"terra\": lambda item: {\n",
    "                0: \"entailment\",\n",
    "                1: \"not_entailment\",\n",
    "                -1: \"not_entailment\",\n",
    "            }[item[\"label\"]],\n",
    "            \"russe\": lambda item: {\n",
    "                0: \"false\",\n",
    "                1: \"true\",\n",
    "                -1: \"true\",\n",
    "            }[item[\"label\"]],\n",
    "            \"rwsd\": lambda item: {\n",
    "                0: \"False\",\n",
    "                1: \"True\",\n",
    "                -1: \"False\",\n",
    "            }[item[\"label\"]],\n",
    "            \"danetqa\": lambda item: {\n",
    "                0: \"false\",\n",
    "                1: \"true\",\n",
    "                -1: \"frue\",\n",
    "            }[item[\"label\"]],\n",
    "            \"parus\": lambda item: item[\"label\"],\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def rucos_gold(self, item):\n",
    "        if self.split == \"test\":\n",
    "            return item[\"answers\"]\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def get_prompt(self, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": self.lidirus_prompt,\n",
    "            \"rcb\": self.rcb_prompt,\n",
    "            \"rucos\": self.rucos_prompt,\n",
    "            \"muserc\": self.muserc_prompt,\n",
    "            \"terra\": self.terra_prompt,\n",
    "            \"danetqa\": self.danetqa_prompt,\n",
    "            \"parus\": self.parus_prompt,\n",
    "            \"russe\": self.russe_prompt,\n",
    "            \"rwsd\": self.rwsd_prompt,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def evaluate_answer(self, result, item):\n",
    "        handlers_map = {\n",
    "            \"lidirus\": self.lidirus_eval,\n",
    "            \"rcb\": self.rcb_eval,\n",
    "            \"rucos\": self.rucos_eval,\n",
    "            \"muserc\": self.muserc_eval,\n",
    "            \"terra\": self.terra_eval,\n",
    "            \"danetqa\": self.danetqa_eval,\n",
    "            \"parus\": self.parus_eval,\n",
    "            \"russe\": self.russe_eval,\n",
    "            \"rwsd\": self.rwsd_eval,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item, result=result)\n",
    "\n",
    "    def save_muserc(self, path_flat, save_path):\n",
    "        with open(path_flat, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [json.loads(item) for item in lines]\n",
    "            real_prediction = {}\n",
    "            for line in lines:\n",
    "                paragraph, question, answer = (\n",
    "                    line[\"idx\"][\"paragraph\"],\n",
    "                    line[\"idx\"][\"question\"],\n",
    "                    line[\"idx\"][\"answer\"],\n",
    "                )\n",
    "                label = line[\"label\"]\n",
    "                if not paragraph in real_prediction:\n",
    "                    real_prediction[paragraph] = {\n",
    "                        \"idx\": paragraph,\n",
    "                        \"passage\": {\n",
    "                            \"questions\": {\n",
    "                                question: {\n",
    "                                    \"idx\": question,\n",
    "                                    \"answers\": [\n",
    "                                        {\n",
    "                                            \"idx\": answer,\n",
    "                                            \"label\": label,\n",
    "                                        }\n",
    "                                    ],\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                    }\n",
    "                else:\n",
    "                    if (\n",
    "                        not question\n",
    "                        in real_prediction[paragraph][\"passage\"][\"questions\"]\n",
    "                    ):\n",
    "                        real_prediction[paragraph][\"passage\"][\"questions\"][question] = {\n",
    "                            \"idx\": question,\n",
    "                            \"answers\": [\n",
    "                                {\n",
    "                                    \"idx\": answer,\n",
    "                                    \"label\": label,\n",
    "                                }\n",
    "                            ],\n",
    "                        }\n",
    "                    else:\n",
    "                        real_prediction[paragraph][\"passage\"][\"questions\"][question][\n",
    "                            \"answers\"\n",
    "                        ].append(\n",
    "                            {\n",
    "                                \"idx\": answer,\n",
    "                                \"label\": label,\n",
    "                            }\n",
    "                        )\n",
    "        real_prediction = list(real_prediction.values())\n",
    "        for item in real_prediction:\n",
    "            item[\"passage\"][\"questions\"] = list(item[\"passage\"][\"questions\"].values())\n",
    "\n",
    "        with open(save_path, \"w\") as f:\n",
    "            for item in real_prediction:\n",
    "                json.dump(item, f, ensure_ascii=False)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config_verbalist = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    max_new_tokens=50,\n",
    "    # no_repeat_ngram_size=15,\n",
    "    repetition_penalty=1.1,\n",
    "    temperature=0.5,\n",
    "    top_k=20,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    ")\n",
    "for name in [\n",
    "    \"lidirus\",\n",
    "    \"rcb\",\n",
    "    \"parus\",\n",
    "    \"muserc\",\n",
    "    \"terra\",\n",
    "    \"russe\",\n",
    "    \"rwsd\",\n",
    "    \"danetqa\",\n",
    "    \"rucos\",\n",
    "]:\n",
    "    evaluation = EvalRussianSuperGlue(\n",
    "        dataset_name=name,\n",
    "        # split=\"valid\",\n",
    "        split=\"test\",\n",
    "        # model_type=\"dummy\",\n",
    "        base_folder=\"verbalist/evaluation/russian_super_glue/valid_evaluations/\",\n",
    "        # eval_name=\"dummy\",\n",
    "        eval_name=\"saiga_mistral\",\n",
    "        # eval_name=\"verbalist_7b_v9_800\",\n",
    "        # debug_mode=True,\n",
    "        # generation_function=partial(\n",
    "        #     RussianSuperGlueModels.chat_gpt,\n",
    "        # ),\n",
    "        # generation_function=partial(\n",
    "        #     RussianSuperGlueModels.saiga_mistral,\n",
    "        #     model=model,\n",
    "        #     generation_config=generation_config\n",
    "        # ),\n",
    "        generation_function=partial(\n",
    "            RussianSuperGlueModels.saiga_mistral,\n",
    "            model=model,\n",
    "            generation_config=generation_config_verbalist,\n",
    "            # bot_token_id=12435,\n",
    "        ),\n",
    "        # generation_function=partial(\n",
    "        #     RussianSuperGlueModels.verbalist_generation,\n",
    "        #     model=model,\n",
    "        #     generation_config=generation_config_verbalist,\n",
    "        #     bot_token_id=12435,\n",
    "        # ),\n",
    "    )\n",
    "\n",
    "    evaluation.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 + 2 + 34 + 14 + 55 + 1 + 4 + 46 mistral_saiga"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
