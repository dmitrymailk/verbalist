{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.25s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT <s> system \n",
      "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
      "<s> user high_quality\n",
      "Почему трава зеленая? </s> \n",
      "<s> bot high_quality\n",
      "Зеленый цвет травы обусловлен наличием хлорофилла - пигмента, который позволяет растениям производить энергию через процесс фотосинтеза. Хлорофилл поглощает световые волны в диапазоне от красного до синего, что создает ощущение зеленого цвета для человеческого глаза.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from verbalist.generation.generation_utils import (\n",
    "    VerbalistConversation,\n",
    "    generate,\n",
    "    VerbalistOpenchatConversation,\n",
    ")\n",
    "\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v9/checkpoint-750/adapter_model\"\n",
    "weights_path = \"verbalist/model/models/verbalist_7b_v10/checkpoint-1800/adapter_model\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v11/checkpoint-800/adapter_model/\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v12/checkpoint-700/adapter_model/\"\n",
    "# weights_path = \"verbalist/model/models/verbalist_7b_v8/checkpoint-1500/adapter_model/\"\n",
    "tokenizer_path = \"verbalist/model/models/verbalist_7b_v10/\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(weights_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # device_map=\"auto\",\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    weights_path,\n",
    "    # torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "inputs = [\"Почему трава зеленая?\"]\n",
    "\n",
    "# inputs = [\n",
    "#     f\"\"\"Предложение 1: Мать моя так хотела учиться, что пробилась на выучку к раввину, хотя евреи учат только мальчиков\n",
    "# Предложение 2: Хотя считается, что у евреев должны быть черные волосы и карие глаза, среди них встречаются голубоглазые блондины\n",
    "# Думай шаг за шагом. Является ли слово евреи одинаковым по значению и смыслу в этих двух предложениях. В ответе напиши 'да' или 'нет'.\n",
    "# \"\"\"\n",
    "# ]\n",
    "\n",
    "# conversation = VerbalistConversation(\n",
    "#     bot_token_id=12435,\n",
    "# )\n",
    "conversation = VerbalistOpenchatConversation()\n",
    "conversation.add_user_message(inputs[0])\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "print(\"PROMPT\", prompt)\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    max_new_tokens=512,\n",
    "    # no_repeat_ngram_size=15,\n",
    "    repetition_penalty=1.1,\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    # do_sample=True,\n",
    ")\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "# print(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE,\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\"role\": \"bot\", \"content\": message})\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, MODEL_NAME, torch_dtype=torch.float16)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "\n",
    "inputs = [\n",
    "    \"Почему трава зеленая?\",\n",
    "    # \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\",\n",
    "]\n",
    "for inp in inputs:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    print(inp)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom mistral open orca ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.46s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dim/mistral-open-orca-ru-4600-step\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"dim/mistral-open-orca-ru-4600-step\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Соберите необходимые материалы: вам понадобится мягкая ткань или ладонь, лакомства, такие как угощения или игрушки, и тихая, удобная обстановка.\n",
      "\n",
      "2. Подойдите к котенку медленно и спокойно: это поможет котенку чувствовать себя в безопасности и не напугать.\n",
      "\n",
      "3. Говорите с котенком мягким, успокаивающим голосом: это поможет котенку чувствовать себя непринужденно и успокоиться в вашем присутствии.\n",
      "\n",
      "4. Медленно и осторожно протяните руку к котенку: дайте ему время, чтобы понюхать и исследовать вашу руку.\n",
      "\n",
      "5. Аккуратно погладьте котенка: начните с мягкого поглаживания его тела, двигаясь в направлении его волос. Будьте осторожны с его ушами, так как они чувствительны.\n",
      "\n",
      "6. Постепенно увеличивайте интенсивность: если котенок чувствует себя комфортно, вы можете немного увеличить давление и скорость, но убедитесь, что он по-прежнему наслаждается процессом.\n",
      "\n",
      "7. Предложите лакомство: предложение угощения или игрушки может помочь котенку ассоциировать вас с положительным опытом.\n",
      "\n",
      "8. Наблюдайте за языком тела котенка: если он расслаблен, наклоняется или заявляет, погладьте его больше. Если они напрягаются, сдерживаются или шипят, прекратите и дайте им немного пространства, прежде чем повторить попытку.\n",
      "\n",
      "9. Завершите сеанс угощением или игрушкой: это поможет котенку связать положительный опыт с вами.\n",
      "\n",
      "10. Постепенно увеличивайте частоту и продолжительность сеансов: по мере того, как котенок будет чувствовать себя более комфортно, вы можете проводить больше времени, гладя его, и делать это чаще.\n",
      "\n",
      "Не забывайте всегда быть терпеливым и понимающим с котенком, так как он может быть неопытен или напуган. Убедитесь, что вы создаете безопасную и комфортную среду для них, чтобы они могли наслаждаться процессом.\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=32000,\n",
    "    pad_token_id=32000,\n",
    "    max_new_tokens=8192,\n",
    "    # no_repeat_ngram_size=15,\n",
    "    repetition_penalty=1.0,\n",
    "    # temperature=0.5,\n",
    "    # top_k=40,\n",
    "    # top_p=0.95,\n",
    "    # do_sample=True,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_orca_ru(instruction):\n",
    "    system = \"Вы помощник ИИ, который помогает людям находить информацию.\"\n",
    "\n",
    "    # prompt = f\"### System:\\n{system}\\n\\n### User:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "\n",
    "    # prompt = f\"<|im_start|>system\\n{system}<|im_end|>\\n<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    # input_ids = tokenizer(prompt)\n",
    "\n",
    "    prompt = [\n",
    "        # {\n",
    "        #     \"role\": \"system\",\n",
    "        #     \"content\": system,\n",
    "        # },\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    def generate(model, tokenizer, prompt, generation_config):\n",
    "        data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "        data = {k: v.to(model.device) for k, v in data.items()}\n",
    "\n",
    "        output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "\n",
    "        output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "\n",
    "        output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "        return output.strip()\n",
    "\n",
    "    return generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "\n",
    "# generate_orca_ru(instruction=\"Почему трава зеленая?\")\n",
    "# generate_orca_ru(instruction=\"Сколько пальцев у человека?\")\n",
    "print(generate_orca_ru(instruction=\"Напиши алгоритм как погладить котика\"))\n",
    "# print(generate_orca_ru(instruction=\"Напиши алгоритм как погладить бегемота.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original mistral open orca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 17.82s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Open-Orca/Mistral-7B-OpenOrca\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAT TEMPLATE\n",
      "<|im_start|>user\n",
      "Напиши алгоритм как погладить котика<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "1. Подготовка: Найдите котика и убедитесь, что она находится в спокойном и комфортном месте.\n",
      "\n",
      "2. Постель: Постель должна быть чистой и приятно пахнуть. Подушечки и постельное белье должны быть мягкими и приятными.\n",
      "\n",
      "3. Уборка: Убедитесь, что есть достаточно еды и воды для котика. Ее еда должна быть свежая и приготовлена согласно ее диетическим требованиям.\n",
      "\n",
      "4. Греть: Подойдите к котике и нажмите ее на спину, чтобы она почувствовала вашу теплоту и любовь.\n",
      "\n",
      "5. Массаж: Генделите котику, начиная с шеи и двигаясь вдо\n",
      "NO TEMPLATE\n",
      "Напиши алгоритм как погладить котика.\n",
      "\n",
      "1. Подобрать котика.\n",
      "2. Помыть котика.\n",
      "3. Побрить котика.\n",
      "4. Помыть котика еще раз.\n",
      "5. Посушить котика.\n",
      "6. Подать котику еду.\n",
      "7. Подать котику воду.\n",
      "8. Поставить котику на удобное место.\n",
      "9. Позаботиться о комфорте котика.\n",
      "10. Проверить здоровье котика.\n",
      "\n",
      "Алгоритм погладить котика:\n",
      "\n",
      "1. Подобрать котика.\n",
      "2. Помыть котика.\n",
      "3. Побрить котика.\n",
      "4. Помыть котика еще раз.\n",
      "5. Посушить котика.\n",
      "6. Подать котику еду.\n",
      "7. Подать котику воду.\n",
      "8. Поставить котику на удобное\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_length=256,\n",
    "    temperature=1.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.0,\n",
    "    # do_sample=True,\n",
    "    use_cache=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_original_orca(instruction):\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "    ]\n",
    "    chat = tokenizer.apply_chat_template(\n",
    "        chat, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    print(chat)\n",
    "\n",
    "    inputs = tokenizer(chat, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)[0]\n",
    "    outputs = outputs[len(inputs[\"input_ids\"][0]) :]\n",
    "    text = tokenizer.decode(outputs)\n",
    "    text = text.replace(\"<|im_end|>\", \"\").strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_original_orca_no_template(instruction):\n",
    "    inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)[0]\n",
    "    outputs = outputs[len(inputs[\"input_ids\"]) :]\n",
    "    text = tokenizer.decode(outputs)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"CHAT TEMPLATE\")\n",
    "print(generate_original_orca(\"Напиши алгоритм как погладить котика\"))\n",
    "print(\"NO TEMPLATE\")\n",
    "print(generate_original_orca_no_template(\"Напиши алгоритм как погладить котика\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuCoLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2787/2787 [03:19<00:00, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.692859705776821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Prompts:\n",
    "    def rucola_prompt(self, item):\n",
    "        sentence1 = item[\"sentence\"]\n",
    "        prompt = f\"\"\"Предложение: {sentence1}\\nКорректно ли данное предложение с точки зрения русского языка? Ответь только \"да\" или \"нет\". \"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class Eval:\n",
    "    def rucola_eval(\n",
    "        self,\n",
    "        item=None,\n",
    "        result: str = None,\n",
    "    ):\n",
    "        answer = None\n",
    "        result = result.lower()[:4]\n",
    "        if \"да\" in result:\n",
    "            answer = 1\n",
    "        elif \"нет\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "\n",
    "class Models:\n",
    "    @staticmethod\n",
    "    def verbalist_generation_1(prompt=None, model=None):\n",
    "        conversation = VerbalistConversation(bot_token_id=12435)\n",
    "        conversation.add_user_message(prompt)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "        # print(\"PROMPT\", prompt)\n",
    "        generation_config = GenerationConfig(\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            pad_token_id=0,\n",
    "            max_new_tokens=512,\n",
    "            # no_repeat_ngram_size=15,\n",
    "            repetition_penalty=1.1,\n",
    "            temperature=0.5,\n",
    "            top_k=40,\n",
    "            top_p=0.95,\n",
    "            # do_sample=True,\n",
    "        )\n",
    "        output = generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            generation_config,\n",
    "        )\n",
    "        # print(\"RESULT\", output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def verbalist_openchat_generation(prompt=None, model=None):\n",
    "        conversation = VerbalistOpenchatConversation()\n",
    "        conversation.add_user_message(prompt)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "        # print(\"PROMPT\", prompt)\n",
    "        generation_config = GenerationConfig(\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            pad_token_id=0,\n",
    "            max_new_tokens=512,\n",
    "            # no_repeat_ngram_size=15,\n",
    "            repetition_penalty=1.1,\n",
    "            temperature=0.5,\n",
    "            top_k=40,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "        output = generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            generation_config,\n",
    "        )\n",
    "        # print(\"RESULT\", output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def saiga_mistral(prompt=None, model=None):\n",
    "        conversation = Conversation()\n",
    "        conversation.add_user_message(prompt)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "        output = generate(model, tokenizer, prompt, generation_config)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def original_orca(prompt=None):\n",
    "        output = generate_original_orca(instruction=prompt)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def mistral_orca_ru(prompt=None):\n",
    "        output = generate_orca_ru(instruction=prompt)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def dummy(self, prompt=None):\n",
    "        tasks_map = {\n",
    "            \"rucola\": \"да\",\n",
    "        }\n",
    "        return tasks_map[self.dataset_name]\n",
    "\n",
    "\n",
    "class EvalRussianSuperGlue(\n",
    "    Prompts,\n",
    "    Eval,\n",
    "    Models,\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name=\"rucola\",\n",
    "        model_type=None,\n",
    "        base_folder=None,\n",
    "        generation_function=None,\n",
    "        split=None,\n",
    "        eval_name=None,\n",
    "        debug_mode=False,\n",
    "    ) -> None:\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = load_dataset(\"RussianNLP/rucola\")\n",
    "        self.split = split\n",
    "\n",
    "        self.dataset = self.dataset[self.split]\n",
    "\n",
    "        self.generation_function = generation_function\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.base_folder = Path(base_folder)\n",
    "        self.eval_name = eval_name\n",
    "\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "        if self.debug_mode:\n",
    "            num = 10\n",
    "            self.dataset = self.dataset.select(range(20, 20 + num))\n",
    "\n",
    "        self.eval_filenames = {\n",
    "            \"rucola\": \"RuCoLA\",\n",
    "        }\n",
    "\n",
    "    def evaluate(self):\n",
    "        task_name = self.eval_filenames[self.dataset_name]\n",
    "        print(task_name)\n",
    "\n",
    "        eval_folder = self.base_folder / f\"{self.eval_name}\" / self.split\n",
    "        eval_folder.mkdir(exist_ok=True, parents=True)\n",
    "        output_file = eval_folder / f\"{task_name}.csv\"\n",
    "\n",
    "        if output_file.is_file() and not self.debug_mode and self.model_type != \"dummy\":\n",
    "            with open(eval_folder / f\"{task_name}.txt\", \"r\") as f:\n",
    "                print(f.read())\n",
    "        else:\n",
    "            predicts = []\n",
    "            ground_true = []\n",
    "\n",
    "            with open(eval_folder / f\"{task_name}.log\", \"w\") as f:\n",
    "                idxs = []\n",
    "\n",
    "                for item in tqdm(self.dataset):\n",
    "                    prompt = self.get_prompt(item=item)\n",
    "                    result = self.get_answer(prompt=prompt)\n",
    "\n",
    "                    print(prompt, file=f)\n",
    "                    print(f\"predict answer = {result}\", file=f)\n",
    "                    print(f\"real answer = {item['label']}\", file=f)\n",
    "\n",
    "                    answer = self.evaluate_answer(item=item, result=result)\n",
    "                    gold_true = self.get_gold_true(item=item)\n",
    "\n",
    "                    predicts.append(answer)\n",
    "                    ground_true.append(gold_true)\n",
    "                    idxs.append(item[\"id\"])\n",
    "\n",
    "                acc = str(accuracy_score(ground_true, predicts))\n",
    "\n",
    "                print(f\"Accuracy: {acc}\")\n",
    "                pd.DataFrame(\n",
    "                    data={\n",
    "                        \"id\": idxs,\n",
    "                        \"acceptable\": predicts,\n",
    "                    }\n",
    "                ).to_csv(output_file, index=False)\n",
    "\n",
    "                with open(eval_folder / f\"{task_name}.txt\", \"w\") as f:\n",
    "                    f.write(acc)\n",
    "\n",
    "    def get_answer(self, prompt):\n",
    "        answer = None\n",
    "        if self.model_type == \"dummy\":\n",
    "            answer = self.dummy(prompt=prompt)\n",
    "        else:\n",
    "            answer = self.generation_function(prompt=prompt)\n",
    "        answer = answer.strip()\n",
    "        return answer\n",
    "\n",
    "    def get_gold_true(self, item):\n",
    "        handlers_map = {\n",
    "            \"rucola\": lambda item: item[\"label\"],\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def get_prompt(self, item):\n",
    "        handlers_map = {\n",
    "            \"rucola\": self.rucola_prompt,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def evaluate_answer(self, result, item):\n",
    "        handlers_map = {\n",
    "            \"rucola\": self.rucola_eval,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item, result=result)\n",
    "\n",
    "\n",
    "for name in [\n",
    "    \"rucola\",\n",
    "]:\n",
    "    evaluation = EvalRussianSuperGlue(\n",
    "        dataset_name=name,\n",
    "        split=\"validation\",\n",
    "        # split=\"test\",\n",
    "        # model_type=\"dummy\",\n",
    "        base_folder=\"verbalist/evaluation/RuCoLA/valid\",\n",
    "        # eval_name=\"dummy\",\n",
    "        # eval_name=\"saiga_mistral\",\n",
    "        eval_name=\"openorca_mistral_ru_4600_alpaca_w_system.load_open_orca_chatml\",\n",
    "        # debug_mode=True,\n",
    "        # generation_function=partial(\n",
    "        #     RussianSuperGlueModels.chat_gpt,\n",
    "        # ),\n",
    "        # generation_function=partial(Models.saiga_mistral, model=model),\n",
    "        # generation_function=partial(Models.verbalist_generation_1, model=model),\n",
    "        # generation_function=partial(Models.original_orca),\n",
    "        generation_function=partial(Models.mistral_orca_ru),\n",
    "    )\n",
    "\n",
    "    evaluation.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
