{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)/adapter_config.json: 100%|██████████| 480/480 [00:00<00:00, 3.74MB/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.53s/it]\n",
      "Downloading adapter_model.bin: 100%|██████████| 54.6M/54.6M [00:00<00:00, 76.7MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.35k/1.35k [00:00<00:00, 11.5MB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 270MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 882kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 96.0/96.0 [00:00<00:00, 921kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading (…)neration_config.json: 100%|██████████| 265/265 [00:00<00:00, 2.18MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 1536,\n",
      "  \"no_repeat_ngram_size\": 15,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.2,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Почему трава зеленая?\n",
      "Вопрос о цвете растений является сложным и многоаспектным. Зеленый цвет у большинства растений обусловлен наличием в них хлорофилла - пигмента, который позволяет растениям совершать фотосинтез. Фотосинтез - это процесс, благодаря которому растения получают энергию из света и преобразуют её в химическую энергию, используя углекислый газ из воздуха и воду. Хлорофилл поглощает световые волны длиной 430-450 нм (синий цвет) и 670-680 нм (красный цвет), что позволяет растениям использовать максимальное количество энергии из света.\n",
      "\n",
      "Хлорофилл имеет структуру, состоящую из двух молекул пигмента, которые образуют димер. Внутри каждой молекулы хлорофилла есть две группы, которые отражают свет: карбоксильные группы и метильные группы. Карбоксильные группы отражают синие и красные волны, а метильные группы отражают зелёные волны. Таким образом, зеленый цвет растений обусловлен тем, что они отражают зеленые волны света, а поглощают остальные цвета спектра.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE,\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\"role\": \"bot\", \"content\": message})\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, MODEL_NAME, torch_dtype=torch.float16)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "\n",
    "inputs = [\n",
    "    \"Почему трава зеленая?\",\n",
    "    # \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\",\n",
    "]\n",
    "for inp in inputs:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    print(inp)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuCoLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2789/2789 [08:40<00:00,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Prompts:\n",
    "    def rucola_prompt(self, item):\n",
    "        sentence1 = item[\"sentence\"]\n",
    "        prompt = f\"\"\"Предложение: {sentence1}\\nКорректно ли данное предложение с точки зрения русского языка? Ответь только \"да\" или \"нет\".\"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class Eval:\n",
    "    def rucola_eval(\n",
    "        self,\n",
    "        item=None,\n",
    "        result: str = None,\n",
    "    ):\n",
    "        answer = None\n",
    "        result = result.lower()\n",
    "        if \"да\" in result:\n",
    "            answer = 1\n",
    "        elif \"не\" in result:\n",
    "            answer = 0\n",
    "        else:\n",
    "            answer = int(not bool(item[\"label\"]))\n",
    "        return answer\n",
    "\n",
    "\n",
    "class Models:\n",
    "    # @staticmethod\n",
    "    # def verbalist_generation_1(prompt=None, model=None):\n",
    "    #     conversation = VerbalistConversation()\n",
    "    #     conversation.add_user_message(prompt)\n",
    "    #     prompt = conversation.get_prompt(tokenizer)\n",
    "    #     # print(\"PROMPT\", prompt)\n",
    "    #     generation_config = GenerationConfig(\n",
    "    #         bos_token_id=1,\n",
    "    #         eos_token_id=2,\n",
    "    #         pad_token_id=0,\n",
    "    #         max_new_tokens=512,\n",
    "    #         # no_repeat_ngram_size=15,\n",
    "    #         repetition_penalty=1.1,\n",
    "    #         temperature=0.5,\n",
    "    #         top_k=40,\n",
    "    #         top_p=0.95,\n",
    "    #         # do_sample=True,\n",
    "    #     )\n",
    "    #     output = generate(\n",
    "    #         model,\n",
    "    #         tokenizer,\n",
    "    #         prompt,\n",
    "    #         generation_config,\n",
    "    #     )\n",
    "    #     # print(\"RESULT\", output)\n",
    "    #     return output\n",
    "\n",
    "    @staticmethod\n",
    "    def saiga_mistral(prompt=None, model=None):\n",
    "        conversation = Conversation()\n",
    "        conversation.add_user_message(prompt)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "        output = generate(model, tokenizer, prompt, generation_config)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def dummy(self, prompt=None):\n",
    "        tasks_map = {\n",
    "            \"rucola\": \"да\",\n",
    "        }\n",
    "        return tasks_map[self.dataset_name]\n",
    "\n",
    "\n",
    "class EvalRussianSuperGlue(\n",
    "    Prompts,\n",
    "    Eval,\n",
    "    Models,\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name=\"rucola\",\n",
    "        model_type=None,\n",
    "        base_folder=None,\n",
    "        generation_function=None,\n",
    "        split=None,\n",
    "        eval_name=None,\n",
    "        debug_mode=False,\n",
    "    ) -> None:\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = load_dataset(\"RussianNLP/rucola\")\n",
    "        self.split = split\n",
    "\n",
    "        self.dataset = self.dataset[self.split]\n",
    "\n",
    "        self.generation_function = generation_function\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.base_folder = Path(base_folder)\n",
    "        self.eval_name = eval_name\n",
    "\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "        if self.debug_mode:\n",
    "            num = 10\n",
    "            self.dataset = self.dataset.select(range(20, 20 + num))\n",
    "\n",
    "        self.eval_filenames = {\n",
    "            \"rucola\": \"RuCoLA\",\n",
    "        }\n",
    "\n",
    "    def evaluate(self):\n",
    "        task_name = self.eval_filenames[self.dataset_name]\n",
    "        print(task_name)\n",
    "\n",
    "        eval_folder = self.base_folder / f\"{self.eval_name}\" / self.split\n",
    "        eval_folder.mkdir(exist_ok=True, parents=True)\n",
    "        output_file = eval_folder / f\"{task_name}.csv\"\n",
    "\n",
    "        if output_file.is_file() and not self.debug_mode and self.model_type != \"dummy\":\n",
    "            with open(eval_folder / f\"{task_name}.txt\", \"r\") as f:\n",
    "                print(f.read())\n",
    "        else:\n",
    "            predicts = []\n",
    "            ground_true = []\n",
    "\n",
    "            with open(eval_folder / f\"{task_name}.log\", \"w\") as f:\n",
    "                idxs = []\n",
    "\n",
    "                for item in tqdm(self.dataset):\n",
    "                    prompt = self.get_prompt(item=item)\n",
    "                    result = self.get_answer(prompt=prompt)\n",
    "\n",
    "                    print(prompt, file=f)\n",
    "                    print(f\"predict answer = {result}\", file=f)\n",
    "                    print(f\"real answer = {item['label']}\", file=f)\n",
    "\n",
    "                    answer = self.evaluate_answer(item=item, result=result)\n",
    "                    gold_true = self.get_gold_true(item=item)\n",
    "\n",
    "                    predicts.append(answer)\n",
    "                    ground_true.append(gold_true)\n",
    "                    idxs.append(item[\"id\"])\n",
    "\n",
    "                acc = str(accuracy_score(ground_true, predicts))\n",
    "\n",
    "                print(f\"Accuracy: {acc}\")\n",
    "\n",
    "                # with open(output_file, \"w\") as f:\n",
    "                #     for idx, predict in zip(idxs, predicts):\n",
    "                #         answer = {\n",
    "                #             \"id\": idx,\n",
    "                #             \"acceptable\": predict,\n",
    "                #         }\n",
    "                #         json.dump(answer, f, ensure_ascii=False)\n",
    "                #         f.write(\"\\n\")\n",
    "                pd.DataFrame(\n",
    "                    data={\n",
    "                        \"id\": idxs,\n",
    "                        \"acceptable\": predicts,\n",
    "                    }\n",
    "                ).to_csv(output_file, index=False)\n",
    "\n",
    "                with open(eval_folder / f\"{task_name}.txt\", \"w\") as f:\n",
    "                    f.write(acc)\n",
    "\n",
    "    def get_answer(self, prompt):\n",
    "        answer = None\n",
    "        if self.model_type == \"dummy\":\n",
    "            answer = self.dummy(prompt=prompt)\n",
    "        else:\n",
    "            answer = self.generation_function(prompt=prompt)\n",
    "        answer = answer.strip()\n",
    "        return answer\n",
    "\n",
    "    def get_gold_true(self, item):\n",
    "        handlers_map = {\n",
    "            \"rucola\": lambda item: item[\"label\"],\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def get_prompt(self, item):\n",
    "        handlers_map = {\n",
    "            \"rucola\": self.rucola_prompt,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item)\n",
    "\n",
    "    def evaluate_answer(self, result, item):\n",
    "        handlers_map = {\n",
    "            \"rucola\": self.rucola_eval,\n",
    "        }\n",
    "        return handlers_map[self.dataset_name](item=item, result=result)\n",
    "\n",
    "\n",
    "for name in [\n",
    "    \"rucola\",\n",
    "]:\n",
    "    evaluation = EvalRussianSuperGlue(\n",
    "        dataset_name=name,\n",
    "        # split=\"validation\",\n",
    "        split=\"test\",\n",
    "        # model_type=\"dummy\",\n",
    "        base_folder=\"verbalist/evaluation/RuCoLA/valid\",\n",
    "        # eval_name=\"dummy\",\n",
    "        eval_name=\"saiga_mistral\",\n",
    "        # debug_mode=True,\n",
    "        # generation_function=partial(\n",
    "        #     RussianSuperGlueModels.chat_gpt,\n",
    "        # ),\n",
    "        generation_function=partial(Models.saiga_mistral, model=model),\n",
    "    )\n",
    "\n",
    "    evaluation.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
