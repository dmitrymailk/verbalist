{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD:  None None\n",
      "BOS:  50256 <|endoftext|>\n",
      "EOS:  50256 <|endoftext|>\n",
      "UNK:  50256 <|endoftext|>\n",
      "SEP:  None None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\"\n",
    ")\n",
    "# inputs = tokenizer('''```python\n",
    "# def print_prime(n):\n",
    "#    \"\"\"\n",
    "#    Print all primes between 1 and n\n",
    "#    \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "# outputs = model.generate(**inputs, max_length=200)\n",
    "# text = tokenizer.batch_decode(outputs)[0]\n",
    "# print(text)\n",
    "print(\"PAD: \", tokenizer.pad_token_id, tokenizer.pad_token)\n",
    "print(\"BOS: \", tokenizer.bos_token_id, tokenizer.bos_token)\n",
    "print(\"EOS: \", tokenizer.eos_token_id, tokenizer.eos_token)\n",
    "print(\"UNK: \", tokenizer.unk_token_id, tokenizer.unk_token)\n",
    "print(\"SEP: \", tokenizer.sep_token_id, tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixFormerSequentialForCausalLM(\n",
       "  (layers): Sequential(\n",
       "    (0): Embedding(\n",
       "      (wte): Embedding(51200, 2048)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (2): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (3): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (4): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (5): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (6): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (7): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (8): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (9): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (10): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (11): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (12): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (13): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (14): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (15): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (16): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (17): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (18): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (19): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (20): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (21): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (22): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (23): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (24): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (25): CausalLMHead(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): CausalLMLoss(\n",
       "    (loss_fct): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.end_token_id,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256,\n",
       " 2836,\n",
       " 198,\n",
       " 40,\n",
       " 716,\n",
       " 2615,\n",
       " 257,\n",
       " 12370,\n",
       " 10586,\n",
       " 422,\n",
       " 12692,\n",
       " 13,\n",
       " 314,\n",
       " 1541,\n",
       " 423,\n",
       " 262,\n",
       " 1762,\n",
       " 6890,\n",
       " 290,\n",
       " 716,\n",
       " 287,\n",
       " 262,\n",
       " 1429,\n",
       " 286,\n",
       " 4566,\n",
       " 870,\n",
       " 262,\n",
       " 18779,\n",
       " 13,\n",
       " 2102,\n",
       " 1312,\n",
       " 1064,\n",
       " 326,\n",
       " 262,\n",
       " 10662,\n",
       " 86,\n",
       " 861,\n",
       " 89,\n",
       " 12461,\n",
       " 3607,\n",
       " 502,\n",
       " 15980,\n",
       " 2356,\n",
       " 13,\n",
       " 314,\n",
       " 481,\n",
       " 779,\n",
       " 262,\n",
       " 10586,\n",
       " 329,\n",
       " 3597,\n",
       " 287,\n",
       " 46932,\n",
       " 11,\n",
       " 308,\n",
       " 2224,\n",
       " 290,\n",
       " 48718,\n",
       " 11,\n",
       " 290,\n",
       " 329,\n",
       " 19617,\n",
       " 8384,\n",
       " 13,\n",
       " 198,\n",
       " 2061,\n",
       " 10586,\n",
       " 12461,\n",
       " 561,\n",
       " 307,\n",
       " 1266,\n",
       " 16662,\n",
       " 329,\n",
       " 502,\n",
       " 30,\n",
       " 220,\n",
       " 50256,\n",
       " 220,\n",
       " 198,\n",
       " 50256,\n",
       " 10214,\n",
       " 198,\n",
       " 37058,\n",
       " 11,\n",
       " 340,\n",
       " 2331,\n",
       " 326,\n",
       " 21227,\n",
       " 12461,\n",
       " 743,\n",
       " 307,\n",
       " 644,\n",
       " 345,\n",
       " 389,\n",
       " 2045,\n",
       " 329,\n",
       " 13,\n",
       " 198,\n",
       " 4342,\n",
       " 389,\n",
       " 617,\n",
       " 10586,\n",
       " 38489,\n",
       " 329,\n",
       " 2972,\n",
       " 779,\n",
       " 2663,\n",
       " 25,\n",
       " 220,\n",
       " 198,\n",
       " 1890,\n",
       " 4141,\n",
       " 25,\n",
       " 347,\n",
       " 13038,\n",
       " 37,\n",
       " 17716,\n",
       " 11,\n",
       " 347,\n",
       " 2634,\n",
       " 7501,\n",
       " 198,\n",
       " 1890,\n",
       " 2679,\n",
       " 25,\n",
       " 21227,\n",
       " 11,\n",
       " 1215,\n",
       " 27605,\n",
       " 198,\n",
       " 1890,\n",
       " 3594,\n",
       " 25,\n",
       " 23809,\n",
       " 40,\n",
       " 1404,\n",
       " 16938,\n",
       " 1581,\n",
       " 11,\n",
       " 360,\n",
       " 20867,\n",
       " 461,\n",
       " 11,\n",
       " 198,\n",
       " 1890,\n",
       " 1430,\n",
       " 278,\n",
       " 25,\n",
       " 6118,\n",
       " 647,\n",
       " 360,\n",
       " 20867,\n",
       " 461,\n",
       " 11,\n",
       " 4319,\n",
       " 5634,\n",
       " 11,\n",
       " 4476,\n",
       " 413,\n",
       " 695,\n",
       " 11,\n",
       " 1195,\n",
       " 38,\n",
       " 5805,\n",
       " 54,\n",
       " 56,\n",
       " 11,\n",
       " 9843,\n",
       " 82,\n",
       " 10094,\n",
       " 198,\n",
       " 6425,\n",
       " 326,\n",
       " 981,\n",
       " 12461,\n",
       " 743,\n",
       " 8676,\n",
       " 284,\n",
       " 15980,\n",
       " 2356,\n",
       " 11,\n",
       " 584,\n",
       " 5087,\n",
       " 588,\n",
       " 262,\n",
       " 9848,\n",
       " 286,\n",
       " 262,\n",
       " 10586,\n",
       " 11,\n",
       " 1994,\n",
       " 31050,\n",
       " 357,\n",
       " 1851,\n",
       " 605,\n",
       " 11,\n",
       " 16021,\n",
       " 11,\n",
       " 290,\n",
       " 6795,\n",
       " 828,\n",
       " 517,\n",
       " 743,\n",
       " 635,\n",
       " 307,\n",
       " 14329,\n",
       " 5087,\n",
       " 13,\n",
       " 18252,\n",
       " 257,\n",
       " 649,\n",
       " 12461,\n",
       " 2753,\n",
       " 640,\n",
       " 290,\n",
       " 3626,\n",
       " 11,\n",
       " 290,\n",
       " 743,\n",
       " 787,\n",
       " 340,\n",
       " 517,\n",
       " 2408,\n",
       " 284,\n",
       " 2099,\n",
       " 319,\n",
       " 584,\n",
       " 34512,\n",
       " 13,\n",
       " 220,\n",
       " 50256,\n",
       " 220,\n",
       " 198,\n",
       " 50256,\n",
       " 2836,\n",
       " 198,\n",
       " 10919,\n",
       " 318,\n",
       " 262,\n",
       " 4621,\n",
       " 286,\n",
       " 347,\n",
       " 13038,\n",
       " 37,\n",
       " 17716,\n",
       " 290,\n",
       " 347,\n",
       " 2634,\n",
       " 7501,\n",
       " 11,\n",
       " 625,\n",
       " 7578,\n",
       " 263,\n",
       " 774,\n",
       " 326,\n",
       " 318,\n",
       " 517,\n",
       " 2219,\n",
       " 287,\n",
       " 4881,\n",
       " 13,\n",
       " 220,\n",
       " 50256,\n",
       " 220,\n",
       " 198,\n",
       " 50256,\n",
       " 10214]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"<s> system\n",
    "Ты — Буквоед, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им. </s> \n",
    "<s> user\n",
    "I am building a mechanical keyboard from scratch. I already have the working hardware and am in the process of configuring the firmware. However i find that the qwertz layout gives me wrist pain. I will use the keyboard for writing in english, german and french, and for coding mainly.\n",
    "What keyboard layout would be best suited for me? </s> \n",
    "<s> bot\n",
    "Generally, it seems that Neo layout may be what you are looking for.\n",
    "Here are some keyboard layouts for various use cases: \n",
    "For French: BvoFrak, Bépo\n",
    "For German: Neo, AdNW\n",
    "For English: DHIATENSOR, Dvorak,\n",
    "For programing: Programmer Dvorak, Evolved, Capewell, QGMLWY, Arensito\n",
    "Note that while layout may contribute to wrist pain, other factors like the angle of the keyboard, key spacing (vertical, horizontal, and depth), more may also be contributing factors. Learning a new layout takes time and effort, and may make it more difficult to type on other keyboards. </s> \n",
    "<s> user\n",
    "what is the advantage of BvoFrak and Bépo, over Azerty that is more common in France. </s> \n",
    "<s> bot\n",
    "BvoFrak and Bépo are alternative French keyboard layouts that aim to improve on the traditional Azerty layout. Some of the claimed advantages include:\n",
    "\n",
    "- BvoFrak aims to minimize finger movement and optimize typing efficiency. It groups letters by finger positions rather than alphabetic order.\n",
    "- Bépo aims to allow for faster typing of common French letters and letter combinations. It also groups letters by finger positions and makes some other rearrangements to commonly used keys.\n",
    "- Both BvoFrak and Bépo proponents argue that Azerty is inefficient and optimized for typewriters, not modern keyboards and typing. However, Azerty is deeply entrenched and the alternative layouts have not achieved widespread adoption.\n",
    "\n",
    "So the potential advantages are increased typing efficiency and comfort, but the alternative layouts have to overcome the momentum of the established Azerty layout. Whether the claimed advantages hold up in practice for most typists is unclear without much broader adoption of the alternative layouts. </s> \n",
    "<s> user\n",
    "Is there any information on how long it takes a typical person to adopt the BvoFrak or Bépo layouts? I worry that the time investment may be a concern, or that I may  make more mistakes when programming during the transition period. </s> \n",
    "<s> bot\n",
    "The time it takes to learn a new keyboard layout can vary depending on several factors, including the individual's previous typing experience and the amount of time they can devote to practice. However, on average, it can take between 2-4 weeks to become proficient in a new keyboard layout.\n",
    "\n",
    "During the transition period, it's natural to make more mistakes when typing, especially when programming or using special characters. It's important to be patient with yourself and allow yourself time to adjust to the new layout. With practice, the new layout will become more natural and comfortable to use, and the speed and accuracy of your typing will improve.\n",
    "\n",
    "Ultimately, the investment of time and effort required to learn a new keyboard layout is a personal decision, and it's important to consider the potential benefits and drawbacks before making a decision. While there may be some initial challenges, many people find that the improved ergonomics and efficiency of a new keyboard layout make the investment well worth it in the long run. </s>\"\"\"\n",
    "\n",
    "tokenizer.encode(\n",
    "    \"\"\"<|endoftext|> user\n",
    "I am building a mechanical keyboard from scratch. I already have the working hardware and am in the process of configuring the firmware. However i find that the qwertz layout gives me wrist pain. I will use the keyboard for writing in english, german and french, and for coding mainly.\n",
    "What keyboard layout would be best suited for me? <|endoftext|> \n",
    "<|endoftext|> bot\n",
    "Generally, it seems that Neo layout may be what you are looking for.\n",
    "Here are some keyboard layouts for various use cases: \n",
    "For French: BvoFrak, Bépo\n",
    "For German: Neo, AdNW\n",
    "For English: DHIATENSOR, Dvorak,\n",
    "For programing: Programmer Dvorak, Evolved, Capewell, QGMLWY, Arensito\n",
    "Note that while layout may contribute to wrist pain, other factors like the angle of the keyboard, key spacing (vertical, horizontal, and depth), more may also be contributing factors. Learning a new layout takes time and effort, and may make it more difficult to type on other keyboards. <|endoftext|> \n",
    "<|endoftext|> user\n",
    "what is the advantage of BvoFrak and Bépo, over Azerty that is more common in France. <|endoftext|> \n",
    "<|endoftext|> bot\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256,\n",
       " 10214,\n",
       " 198,\n",
       " 37058,\n",
       " 11,\n",
       " 340,\n",
       " 2331,\n",
       " 326,\n",
       " 21227,\n",
       " 12461,\n",
       " 743,\n",
       " 307,\n",
       " 644,\n",
       " 345,\n",
       " 389,\n",
       " 2045,\n",
       " 329,\n",
       " 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\n",
    "    \"\"\"<|endoftext|> bot\n",
    "Generally, it seems that Neo layout may be what you are looking for.\"\"\",\n",
    "    add_special_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256, 10214]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"\"\"<|endoftext|> bot\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    'I was told in a Latin class that the name Christopher has Greek roots that mean \"one who carries Christ\". I assume that the Latin connection here is fero, which is the verb to carry. \\nWith that in mind, does the name Jennifer have a similar derivation? If so what would she be carrying?',\n",
    "    'It\\'s a perfect example of a false cognate pair that is created by a sort of linguistic homogenization. When we come across foreign words, we pronounce them with our own sounds, mapping a foreign sound to one natural to our own ear that seems close enough for our purposes. Unrelated sounds and syllables from unrelated languages might in this way each get mapped to a single native sound/syllable. In this way, we come to hear the sounds and syllables as related when they are not.\\n\\nMesoamerican languages, for instance, did not have a \"v\" sound. Consequently, when learning Spanish, the natives of Central America had a devil of time with the fairly common v sound and ended up usually pronouncing it as a b. The b sound is related to the v sound, so it wasn\\'t a subtle shift. In consequence, very often to this day words like \"votar\" (to vote) get pronounced identically with words like \"botar\" (to throw out) giving rise to numerous puns. When native Spanish speakers from Latin America learn English, they are often heard saying things like \"Thank you bery much,\" as a result. But \"bery,\" and its false homophones \"berry\" and \"bury\" have no shared ancestry.\\n\\nAs a last point of interest, consider English spelling. It is so difficult precisely because speakers of English and the academic sources that safeguard the English language (Oxford English Dictionary, Merriam-Webster Dictionary, etc.) have made a general decision to preserve as best as possible the original spelling of the words in their original languages, making only occasional modifications to bring them into some kind of conventional conformity. So the Greek suffix -φορος is conventionally transliterated as -pher, whereas the ending of Gwenhwyfar gets transliterated as -fer and the ending of aquifer stays as the Romans wrote it: -fer. This makes our spelling a bear to get a handle of, but it also locks the secret of most words\\' origins right into the words themselves. That means, once one has understood a bit of Latin, Greek, some common Celtic endings and a bit of German, one can almost always discern the origins of the word just by looking at it. Of course, it also means that we have to spend years and years memorizing spelling word lists and still relying on spellchecker far more than we care to admit.',\n",
    "    \"I thought Spanish v was pronounced like b in Castilian as well, not as our v? Or is this about a more subtle difference? I can hear the Columbian guy pronouncin the v's in *vivir* like our v, but the Spaniard seems to be ponouncing it like our b.\",\n",
    "    'Good ear! A reverse flow happens too. We commonly think of \"Spain\" Spanish as \"older\" than Latin American Spanish, and so on. When we look at the idea, we see it is absurd. Spain isn\\'t older than Latin America, and its Spanish isn\\'t either, even though it was spoken there first. LatAm Spanish branched from Castilian, and they both continue to develop alongside each other. The branching happened relatively late in their development and as reliable means of communication were becoming available. Things trendy in the New World spread home, and purists in the New World insist upon the older ways!\\n\\nIn fact, before the 16th century, it never would have been pronounced as a \"b\" at all. Researchers reconstructed historical pronunciations from times that did not have sound recording by examining spelling errors from earlier times. It was noted that spelling errors often tend to simplify spellings and represent words as they are spoken Ten bucks says that the common Latin American misspellings surrounding v/b are entirely absent from before the 16th c.\\n\\nBesides, the v/b misspellings *are* common. Native speakers who are not well educated *very* often make these misspellings - not necessarily in very common or easy words, like *vivir*, but certainly in difficult words. Sometimes, there is overcorrection as well, i.e., words that ought to be spelled with a *b* are spelled with a *v*.',\n",
    "    \"Ah, that is very interesting. I have always wondered about the Spanish shift to b. The pronunciation of Ancient Greek is indeed quite difficult to trace back. Besides spelling errors, we also have prosody (how rhythm and the length of syllables worked in poetry) to inform us about it, and the comments of Greek grammarians on their own language.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, ]\n",
    "a[: len(a) - len(a) % 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len([1, 2, 3]) / 2)+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
