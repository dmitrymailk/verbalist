{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['12'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.from_list([{\"12\": 123}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "datasets_config = [\n",
    "    {\"name\": \"dim/oasst_en\", \"status\": \"ok\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/oasst_ru\", \"status\": \"ok\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/lima\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/logic_tasks_ru\", \"status\": \"ok\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/wikihow_en\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/wikihow_ru\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/essayforum_writing_prompts_6k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/sharegpt_short_ru\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/openreview_prompts_65\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/roleplay_instruct_v2_final\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/kinomania_scripts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/bugurt_thread_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/russian_lyrics_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/ru_instruct_gpt4\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/gpt_roleplay_realm\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/ultrachat_ru\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/scitldr\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/linux_man_pages_tldr_summarized\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/dolphin_ru_3k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/runne_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/lurk_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/panorama_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/resh_edu_short_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/databricks_dolly_15k_ru\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/databricks_dolly_15k_en\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/grammarly_coedit\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/kinopoisk_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/medical_qa_ru_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/joke_explaination_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/oa_stackexchange_200k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/scale_helpful_no_math\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/law_stackexchange_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/ficbook_prompts_best_10k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/azbyka_logic_ru\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/povarenok\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/AO3_fandom_chatbot_1to1\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/habr_prompts_5k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/what_where_when_50k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/competition_math\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/sharegpt_short_en_30k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/ru_turbo_alpaca_evol_instruct\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/ru_turbo_saiga\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/bugurt_completion_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/tldr_17_50k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/grade_school_math_instructions\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/tldr_news\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/grade_school_math_instructions_ru\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/dialogsum\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/HC3_ru\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/horoscopes_ru_10k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/yandex_q_200k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/panorama_prompts_10k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/leetcodesolutions_en_2k\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/forum_uristov_rf_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/dialogsum_ru\", \"status\": \"all\", \"test_size\": 1},\n",
    "    # {\"name\": \"dim/huggingartists_prompts\", \"status\": \"all\", \"test_size\": 1},\n",
    "]\n",
    "\n",
    "all_datasets = {}\n",
    "for item in tqdm(datasets_config):\n",
    "    name = item[\"name\"]\n",
    "    dataset_temp = load_dataset(name)[\"train\"]\n",
    "    # all_datasets.append({\n",
    "    #     \"name\": name,\n",
    "    #     \"dataset\": dataset_temp\n",
    "    # })\n",
    "    all_datasets[name] = dataset_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.DatasetDict\n",
    "dataset = DatasetDict(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dim/oasst_en'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversation_text'],\n",
       "    num_rows: 3141\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"dim/oasst_en\"].select_columns([\"conversation_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All datasets in `DatasetDict` should have the same features but features for 'dim/oasst_ru' and 'dim/lima' don't match: {'conversation_ids': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'conversation_text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'status': Value(dtype='string', id=None)} != {'conversations': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'source': Value(dtype='string', id=None)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kosenko/verbalist/verbalist/datasets/combine_datasets/combine_datasets.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/kosenko/verbalist/verbalist/datasets/combine_datasets/combine_datasets.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dataset\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m'\u001b[39;49m\u001b[39mdim/verbalist_dataset\u001b[39;49m\u001b[39m'\u001b[39;49m, private\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/dataset_dict.py:1626\u001b[0m, in \u001b[0;36mDatasetDict.push_to_hub\u001b[0;34m(self, repo_id, config_name, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1622\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease provide one `num_shards` per dataset in the dataset dictionary, e.g. \u001b[39m\u001b[39m{{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: 128, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: 4}}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1623\u001b[0m     )\n\u001b[1;32m   1625\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_values_type()\n\u001b[0;32m-> 1626\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_values_features()\n\u001b[1;32m   1627\u001b[0m total_uploaded_size \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1628\u001b[0m total_dataset_nbytes \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/dataset_dict.py:53\u001b[0m, in \u001b[0;36mDatasetDict._check_values_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m item_a, item_b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(items[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], items[\u001b[39m1\u001b[39m:]):\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m item_a[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mfeatures \u001b[39m!=\u001b[39m item_b[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mfeatures:\n\u001b[0;32m---> 53\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAll datasets in `DatasetDict` should have the same features but features for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mitem_a[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mitem_b[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match: \u001b[39m\u001b[39m{\u001b[39;00mitem_a[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mfeatures\u001b[39m}\u001b[39;00m\u001b[39m != \u001b[39m\u001b[39m{\u001b[39;00mitem_b[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mfeatures\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: All datasets in `DatasetDict` should have the same features but features for 'dim/oasst_ru' and 'dim/lima' don't match: {'conversation_ids': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'conversation_text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'status': Value(dtype='string', id=None)} != {'conversations': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'source': Value(dtype='string', id=None)}"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(\"dim/verbalist_dataset\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3141/3141 [00:00<00:00, 262107.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3140/3140 [00:00<00:00, 241034.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1030/1030 [00:00<00:00, 140291.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 99/99 [00:00<00:00, 29503.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1995/1995 [00:00<00:00, 116562.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2058/2058 [00:00<00:00, 83954.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6361/6361 [00:00<00:00, 254443.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 253/253 [00:00<00:00, 54212.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 150/150 [00:00<00:00, 11461.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3000/3000 [00:00<00:00, 649139.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 27/27 [00:00<00:00, 2805.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 223/223 [00:00<00:00, 78493.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 43/43 [00:00<00:00, 15672.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14222/14222 [00:00<00:00, 657221.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8700/8700 [00:00<00:00, 322850.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 56030.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3229/3229 [00:00<00:00, 535524.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 481/481 [00:00<00:00, 93280.02 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3000/3000 [00:00<00:00, 228559.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 537/537 [00:00<00:00, 114739.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5671/5671 [00:00<00:00, 86431.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5000/5000 [00:00<00:00, 264872.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2106/2106 [00:00<00:00, 154054.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14914/14914 [00:00<00:00, 501056.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 15011/15011 [00:00<00:00, 995017.03 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 82466/82466 [00:00<00:00, 2466045.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 36591/36591 [00:00<00:00, 337042.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 80101/80101 [00:00<00:00, 1206079.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 364/364 [00:00<00:00, 113934.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200000/200000 [00:00<00:00, 1063174.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 17095/17095 [00:00<00:00, 228771.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 24343/24343 [00:00<00:00, 444485.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 59368.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 480/480 [00:00<00:00, 139258.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 46500/46500 [00:00<00:00, 296636.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 614/614 [00:00<00:00, 161258.78 examples/s]\n",
      "Saving the dataset (3/3 shards): 100%|██████████| 5000/5000 [00:01<00:00, 3289.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 50000/50000 [00:00<00:00, 908259.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 7500/7500 [00:00<00:00, 749446.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 29597/29597 [00:00<00:00, 375731.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 47793/47793 [00:00<00:00, 492563.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 37731/37731 [00:00<00:00, 424994.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5000/5000 [00:00<00:00, 479546.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 50000/50000 [00:00<00:00, 240516.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8792/8792 [00:00<00:00, 715322.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 7138/7138 [00:00<00:00, 770173.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 7473/7473 [00:00<00:00, 547102.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12460/12460 [00:00<00:00, 463027.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 24322/24322 [00:00<00:00, 208826.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 621221.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200000/200000 [00:00<00:00, 795045.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 11024/11024 [00:00<00:00, 351184.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2048/2048 [00:00<00:00, 227686.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1849/1849 [00:00<00:00, 389966.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12460/12460 [00:00<00:00, 569683.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 64006/64006 [00:00<00:00, 570095.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"./test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    \"MongoDB _id beyond 9999999\\nI have a collection with more then 10M documents. I am trying to get the biggest _id form that Mongo collection with PHP\\n \\n \\n $cursor = $collection->find(array())->sort(array('_id'=>-1))->limit(1);\\n foreach ($cursor as $doc)\\n {\\n echo $doc['_id'];\\n }\\n \\n\\nI am getting 9999999 as the answer every time.\\n\\nThis also doesn't work\\n \\n \\n $cursor = $collection->find(array('_id'=>array('$gte'=>'5000000')))->sort(array('_id'=>-1))->limit(1);\\n \\n\\nI also tried with the Aggregate \\n \\n \\n $query = array(\\n array('$match'=>array('_id'=>array('$gte'=>'5000000'))),\\n array('$group'=>array('_id'=>'','max_id'=>array('$max'=>'$_id'))),\\n \\n );\\n $res = $collection->aggregate($query);\\n print_r($res);\\n \\n\\nSame 9999999 pops up but this time it takes 6-7 seconds to compute.\\n\\nIt seems to me that there is a limit in MongoDB which prevents me from doing beyond 9999999\",\n",
    "    \"You don't directly show it, but it appears that your `_id` values are numeric strings, not numbers. Strings sort alphabetically, so `'9999999'` (and even `'9'`) would come before `'10000000'` in your descending sort.\\n\\nYour best option is probably to update your docs to use a numeric data type for `_id` and then they will sort as you're expecting.\",\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
