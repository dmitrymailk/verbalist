{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5397/5397 [2:02:07<00:00,  1.36s/it]  \n"
     ]
    }
   ],
   "source": [
    "class WhatWhereWhenContentParser:\n",
    "    def __init__(self, urls: list[str]) -> None:\n",
    "        self.urls = urls\n",
    "\n",
    "        self.dataset = []\n",
    "\n",
    "    def parse(self):\n",
    "        for url in tqdm(self.urls):\n",
    "            response = requests.get(url=url)\n",
    "            soup = BeautifulSoup(response.text)\n",
    "            all_tasks = soup.find_all(\"div\", class_=\"question\")\n",
    "\n",
    "            # editor = soup.find_all(\"div\", class_=\"editor\")\n",
    "            # if len(editor) > 0:\n",
    "            #     editor = editor[0].text\n",
    "            # else:\n",
    "            #     editor = soup.find(\"h1\", \"info\")\n",
    "\n",
    "            # title = soup.find(\"h1\", \"title\").text\n",
    "            # date = soup.find_all(\"p\")[1].text[5:].strip()\n",
    "\n",
    "            for task in all_tasks:\n",
    "                has_image = len(task.find_all(\"img\")) > 0\n",
    "                has_razdatka = len(task.find_all(\"div\", class_=\"razdatka\")) > 0\n",
    "                has_razdatka2 = \"Раздаточный материал\" in task.text\n",
    "                has_explanation = len(task.find_all(\"strong\", class_=\"Comments\")) > 0\n",
    "                if (\n",
    "                    not has_image\n",
    "                    and not has_razdatka\n",
    "                    and has_explanation\n",
    "                    and not has_razdatka2\n",
    "                ):\n",
    "                    answer = task.find_all(\"p\")[1].text.replace(\"\\nОтвет:\", \"\").strip()\n",
    "\n",
    "                    question = task.find_all(\"p\")[0].text\n",
    "                    question = re.sub(r\"Вопрос \\d+\\:\", \"\", question).strip()\n",
    "\n",
    "                    explanation = (\n",
    "                        task.find(\"strong\", class_=\"Comments\")\n",
    "                        .find_parent(\"p\")\n",
    "                        .text.replace(\"Комментарий:\", \"\")\n",
    "                        .strip()\n",
    "                    )\n",
    "\n",
    "                    self.dataset.append(\n",
    "                        {\n",
    "                            # \"title\": title,\n",
    "                            # \"date\": date,\n",
    "                            # \"editor\": editor,\n",
    "                            \"question\": question,\n",
    "                            \"answer\": answer,\n",
    "                            \"explanation\": explanation,\n",
    "                            \"url\": url,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        save_path = \"./verbalist/datasets/what_where_when/what_where_when_questions.csv\"\n",
    "        pd.DataFrame(data=self.dataset).to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "links_dataset = pd.read_csv(\n",
    "    \"./verbalist/datasets/what_where_when/what_where_when_links.csv\"\n",
    ")\n",
    "links_dataset = links_dataset[\"link\"].tolist()\n",
    "\n",
    "parser = WhatWhereWhenContentParser(urls=links_dataset)\n",
    "parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 228/228 [00:00<00:00, 290.39ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:21<00:00, 21.54s/it]\n",
      "Downloading metadata: 100%|██████████| 33.0/33.0 [00:00<00:00, 237kB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\n",
    "    \"./verbalist/datasets/what_where_when/what_where_when_questions.csv\"\n",
    ")\n",
    "\n",
    "dataset[\"uuid\"] = dataset[\"explanation\"].apply(lambda x: str(uuid.uuid4()))\n",
    "dataset = dataset.astype(\"str\")\n",
    "dataset = dataset.to_dict(\"records\")\n",
    "dataset = Dataset.from_list(dataset)\n",
    "# dataset.push_to_hub(\"dim/what_where_when_ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [01:14<00:00,  3.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class WhatWhereWhenLinksParser:\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        self.urls = []\n",
    "\n",
    "        self.dataset = []\n",
    "        self.domain = \"https://db.chgk.info\"\n",
    "\n",
    "    def parse(\n",
    "        self,\n",
    "    ):\n",
    "        pages_amount = 270\n",
    "        for page_num in tqdm(range(pages_amount)):\n",
    "            url = f\"https://db.chgk.info/last?page={page_num}\"\n",
    "            response = requests.get(url=url)\n",
    "            soup = BeautifulSoup(response.text)\n",
    "\n",
    "            odd_links = soup.find_all(\"tr\", \"odd\")\n",
    "            even_links = soup.find_all(\"tr\", \"even\")\n",
    "            links = even_links + odd_links\n",
    "\n",
    "            for link in links:\n",
    "                document_link = link.find_all(\"a\")[0][\"href\"]\n",
    "                document_link = f\"{self.domain}{document_link}\"\n",
    "                self.dataset.append(document_link)\n",
    "\n",
    "\n",
    "links_parser = WhatWhereWhenLinksParser()\n",
    "links_parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5397"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links_parser.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [{\"link\": item} for item in links_parser.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=dataset).to_csv(\n",
    "    \"./verbalist/datasets/what_where_when/what_where_when_links.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create subset for instruction tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosenko/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/kosenko/.cache/huggingface/datasets/dim___parquet/dim--what_where_when_ru-fc08f0f8242e77d0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 107M/107M [00:01<00:00, 81.9MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 222.26it/s]\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/kosenko/.cache/huggingface/datasets/dim___parquet/dim--what_where_when_ru-fc08f0f8242e77d0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 179.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dim/what_where_when_ru\")\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=3000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 29.37ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2515768"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'].to_csv(\"./verbalist/datasets/what_where_when/what_where_when_selected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 24.12ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\n",
      "Downloading metadata: 100%|██████████| 376/376 [00:00<00:00, 2.53MB/s]\n",
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "dataset['test'].push_to_hub('dim/what_where_when_3k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/kosenko/.cache/huggingface/datasets/dim___parquet/dim--what_where_when_3k-15e437af05cc5b3b/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 864.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dim/what_where_when_3k\")\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Дуплет.\\n\\n\\xa0\\xa0\\xa0\\xa01. Джон Стюарт Милль считал, что ОНА стала первым примером ущерба,\\nкоторый может нанести общество, не способное предоставить полную свободу\\nречи, мысли и действия. Назовите ЕЕ двумя словами, начинающимися на одну\\nи ту же букву.\\n\\n\\xa0\\xa0\\xa0\\xa02. ОНА произошла в тот день, когда \"Коринтианс\" выиграл чемпионат\\nБразилии, и, таким образом, пожелание сбылось. Назовите ЕЕ двумя\\nсловами, начинающимися на одну и ту же букву.',\n",
       " 'answer': '1. Смерть Сократа.\\n\\n\\xa0\\xa0\\xa0\\xa02. Смерть Сократеса.',\n",
       " 'explanation': '1. Одной из причин смертного приговора Сократу являлось обвинение в\\nбогохульстве.\\n\\n\\xa0\\xa0\\xa0\\xa02. Известный футболист Сократес однажды выразил желание уйти из жизни\\nв воскресенье, когда \"Коринтианс\" выиграет трофей.',\n",
       " 'url': 'https://db.chgk.info/tour/kaluga16',\n",
       " 'uuid': 'f1e0b73c-abc0-47f3-9d78-d73b2a38bea9'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дуплет.\n",
      "\n",
      "    1. Джон Стюарт Милль считал, что ОНА стала первым примером ущерба,\n",
      "который может нанести общество, не способное предоставить полную свободу\n",
      "речи, мысли и действия. Назовите ЕЕ двумя словами, начинающимися на одну\n",
      "и ту же букву.\n",
      "\n",
      "    2. ОНА произошла в тот день, когда \"Коринтианс\" выиграл чемпионат\n",
      "Бразилии, и, таким образом, пожелание сбылось. Назовите ЕЕ двумя\n",
      "словами, начинающимися на одну и ту же букву.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Одной из причин смертного приговора Сократу являлось обвинение в\n",
      "богохульстве.\n",
      "\n",
      "    2. Известный футболист Сократес однажды выразил желание уйти из жизни\n",
      "в воскресенье, когда \"Коринтианс\" выиграет трофей.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Смерть Сократа.\n",
      "\n",
      "    2. Смерть Сократеса.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
